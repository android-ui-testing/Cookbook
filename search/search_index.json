{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"basics/","text":"Basics","title":"Basics"},{"location":"basics/#basics","text":"","title":"Basics"},{"location":"adoption/companies_experience/","text":"Companies experience Looking to how other companies implemented UI/Snapshot testing may help you to choose proper tools for your project and save your time. We would be really grateful if you could contribute and share your experience directly to this page, to help other people. Revolut UI testing Write: Kaspresso Who write: Android Engineers Runner: Marathon locally and on the CI Where: Headless emulators in Docker (Avito Image) How often: Each 4h and before each release Network: Mock, by Custom OkReplay Test report: Allure Other: We use custom OkReplay to achieve requests indexing, and the same request time as it was while recording. We're going to open-source this solution Snapshot testing Tools: Screenshot tests for Android How often to run: Each commit to the Design System module Other: We write snapshot tests per each component in the design system in all possible states, we don't write them for screens which implemented by using that components. Kaspersky UI testing Write: Kaspresso Who write: QA and developers Runner: AndroidJUnitRunner locally and on CI we use Marathon + custom tooling on top Where: On CI: Emulators (we use custom Docker container) and real devices (custom integration with STF ) How often: Each pull request (functional tests), before the release (e2e tests) and nightly (e2e tests) Test report: On CI we use custom internal solution Snapshot testing Tools: Kaspresso How often: Many times per new feature to check new strings and translations Check24 UI testing Write: Kaspresso Who write: developers Runner: AndroidJUnitRunner with Android Orchestrator Where: On CI: real devices How often: at noon and at night Test report: Junit4 Snapshot testing Tools: Shot Who write: developers Runner: TestButler + Composer for test sharding (Shot support out of the box) Where: On CI: emulators How often: On every PR Test report: Shot report, which includes image diffs when tests fail Headhunter (hh.ru) UI testing Write: Kaspresso wrapped with custom DSL for creating a test data Who writes: QA with support of Android Engineers Runner: Marathon on the CI Where: Headless emulators in Docker (Custom Image) at k8s How often: Every night on every Portfolio branch(protected branch for each business feature) and develop; Every PR to develop. Test data: End2End testing with test stands Test report: Allure Test stability monitoring: Custom tool for success rate visualization of each test between CI runs; Grafana for common graphs. Delivery Club UI testing Write: Kaspresso Who: QA and developers Runner: Delivery Club fork of Avito Runner , Argo Workflows Where: Redroid AiC , Redroid in DockerHub , Fork of Avito Emulator , Fork of Avito Emulator in DockerHub How often: Each commit for Courier App and Consumer App, Before regress testing Network: MockWebServer Test report: Kaspresso Allure Integration + Avito Runner Integration Other: Run Marathon in cloud Auto.ru UI testing & Screenshot testing Write: Espresso, Screenshot tests from facebook Who: Android Developers and QA Engineers Runner: Custom runner based on Android Orchestrator Where: Emulators How often: Each commit, nightly and before release Network: MockWebServer Test report: Allure Test monitoring: Collecting allure info in Postgres db and displaying it in DataLens , finding flaky packages, common errors, alerts, etc. CFT UI testing Write: Kaspresso Who: Android Developers and QA Engineers Runner: Marathon on the CI Where: Emulators How often: On every PR and one time per day on Main branch Network: Custom mockapi server Test report: Allure Test monitoring: Using Allure reports and Grafana monitoring for stable, resources work` \u0421\u0438\u0442\u0438\u043c\u043e\u0431\u0438\u043b UI testing Write: Kaspresso Who writes: QA Automation and QA Engineers Runner: Marathon on the CI Where: Emulators How often: Each merge request Test report: Allure Network: Custom mock system BetterMe UI testing Write: Espresso, UIAutomator, Ultron , compose ui-test Who write: QA Automation Engineers Runner: AllureAndroidJUnitRunner Where: on CI: emulators How often: nightly and before each release Test report: Allure TestOps Test monitoring: Allure TestOps","title":"Companies experience"},{"location":"adoption/companies_experience/#companies-experience","text":"Looking to how other companies implemented UI/Snapshot testing may help you to choose proper tools for your project and save your time. We would be really grateful if you could contribute and share your experience directly to this page, to help other people.","title":"Companies experience"},{"location":"adoption/companies_experience/#revolut","text":"UI testing Write: Kaspresso Who write: Android Engineers Runner: Marathon locally and on the CI Where: Headless emulators in Docker (Avito Image) How often: Each 4h and before each release Network: Mock, by Custom OkReplay Test report: Allure Other: We use custom OkReplay to achieve requests indexing, and the same request time as it was while recording. We're going to open-source this solution Snapshot testing Tools: Screenshot tests for Android How often to run: Each commit to the Design System module Other: We write snapshot tests per each component in the design system in all possible states, we don't write them for screens which implemented by using that components.","title":"Revolut"},{"location":"adoption/companies_experience/#kaspersky","text":"UI testing Write: Kaspresso Who write: QA and developers Runner: AndroidJUnitRunner locally and on CI we use Marathon + custom tooling on top Where: On CI: Emulators (we use custom Docker container) and real devices (custom integration with STF ) How often: Each pull request (functional tests), before the release (e2e tests) and nightly (e2e tests) Test report: On CI we use custom internal solution Snapshot testing Tools: Kaspresso How often: Many times per new feature to check new strings and translations","title":"Kaspersky"},{"location":"adoption/companies_experience/#check24","text":"UI testing Write: Kaspresso Who write: developers Runner: AndroidJUnitRunner with Android Orchestrator Where: On CI: real devices How often: at noon and at night Test report: Junit4 Snapshot testing Tools: Shot Who write: developers Runner: TestButler + Composer for test sharding (Shot support out of the box) Where: On CI: emulators How often: On every PR Test report: Shot report, which includes image diffs when tests fail","title":"Check24"},{"location":"adoption/companies_experience/#headhunter-hhru","text":"UI testing Write: Kaspresso wrapped with custom DSL for creating a test data Who writes: QA with support of Android Engineers Runner: Marathon on the CI Where: Headless emulators in Docker (Custom Image) at k8s How often: Every night on every Portfolio branch(protected branch for each business feature) and develop; Every PR to develop. Test data: End2End testing with test stands Test report: Allure Test stability monitoring: Custom tool for success rate visualization of each test between CI runs; Grafana for common graphs.","title":"Headhunter (hh.ru)"},{"location":"adoption/companies_experience/#delivery-club","text":"UI testing Write: Kaspresso Who: QA and developers Runner: Delivery Club fork of Avito Runner , Argo Workflows Where: Redroid AiC , Redroid in DockerHub , Fork of Avito Emulator , Fork of Avito Emulator in DockerHub How often: Each commit for Courier App and Consumer App, Before regress testing Network: MockWebServer Test report: Kaspresso Allure Integration + Avito Runner Integration Other: Run Marathon in cloud","title":"Delivery Club"},{"location":"adoption/companies_experience/#autoru","text":"UI testing & Screenshot testing Write: Espresso, Screenshot tests from facebook Who: Android Developers and QA Engineers Runner: Custom runner based on Android Orchestrator Where: Emulators How often: Each commit, nightly and before release Network: MockWebServer Test report: Allure Test monitoring: Collecting allure info in Postgres db and displaying it in DataLens , finding flaky packages, common errors, alerts, etc.","title":"Auto.ru"},{"location":"adoption/companies_experience/#cft","text":"UI testing Write: Kaspresso Who: Android Developers and QA Engineers Runner: Marathon on the CI Where: Emulators How often: On every PR and one time per day on Main branch Network: Custom mockapi server Test report: Allure Test monitoring: Using Allure reports and Grafana monitoring for stable, resources work`","title":"CFT"},{"location":"adoption/companies_experience/#_1","text":"UI testing Write: Kaspresso Who writes: QA Automation and QA Engineers Runner: Marathon on the CI Where: Emulators How often: Each merge request Test report: Allure Network: Custom mock system","title":"\u0421\u0438\u0442\u0438\u043c\u043e\u0431\u0438\u043b"},{"location":"adoption/companies_experience/#betterme","text":"UI testing Write: Espresso, UIAutomator, Ultron , compose ui-test Who write: QA Automation Engineers Runner: AllureAndroidJUnitRunner Where: on CI: emulators How often: nightly and before each release Test report: Allure TestOps Test monitoring: Allure TestOps","title":"BetterMe"},{"location":"adoption/where_to_begin/","text":"Where to begin","title":"Where to begin"},{"location":"adoption/where_to_begin/#where-to-begin","text":"","title":"Where to begin"},{"location":"basics/instrumented_testing_basics/","text":"Instrumented testing Instrumented tests are usual junit tests, but with one peculiarity: They can be launched on Android device only. Using them, you may check how your application communicates with Android OS. However, they are written and executed much slower. Tests location Unlike regular jvm tests, instrumented tests located in different src: androidTest Why different src? You need to write code which describes a communication between your application and android device. For instance, clicking some buttons and checking that particular content has been shown, etc. All that code should be compiled, somehow installed on the device and should make yours application to trigger the checks you need. This code can't be a part of testSrc , because of that case we run it on JVM . There is no information about everything related to android. Every Android SDK class instance used in JVM test will be stubbed. In androidSrc we have a real knowledge about Android SDK How tests run under the hood To be able to run your tests on CI and make it a part of CD , it's really important to understand how it works under the hood. 2.1 Build To test our application, we need to build it. We can do that with gradle: # It will build an apk file located in app/build/outputs/debug/debug.apk ./gradlew assembleDebug However, it's not enough for us. Remember? We also need to take care of the code we write in androidTestSrc . It also should be built and will be represented as an apk: # It will build an apk file located in app/build/outputs/debug/debug.apk ./gradlew assembleDebug # It will build an apk file located in app/build/outputs/androidTest/instrumented.apk ./gradlew assembleDebugAndroidTest We've got 2 apks: Application and Test application , which can communicate with Application 2.2 Install To do that, we need to use adb adb install debug.apk adb install instrumented.apk 2.3 Run For running instrumented tests, AndroidJunitRunner is responsible As an input, you need to provide tests you want to run. As an output, tests result will be provided. All you need to do it's to execute adb command: adb shell am instrument -w -m -e debug false \\ -e class 'com.alexbykov.myapplication.ExampleInstrumentedTest#myTest' \\ com.alexbykov.myapplication.test/androidx.test.runner.AndroidJUnitRunner We need to provide some information about tests needed to be launched: particular class, class with method or package. After execution, you may find junit report in app/build/test-results/ It's also possible to define your own instrumented arguments and get them in tests: //add -e myKey \"test\" to adb command InstrumentationRegistry . getArguments (). getString ( \"myKey\" , \"value\" ) Official documentation Instrumented testing types UI Screenshot or Snapshot Migration They are not a replacement of each other, they are complement to each other","title":"Instrumented testing"},{"location":"basics/instrumented_testing_basics/#instrumented-testing","text":"Instrumented tests are usual junit tests, but with one peculiarity: They can be launched on Android device only. Using them, you may check how your application communicates with Android OS. However, they are written and executed much slower.","title":"Instrumented testing"},{"location":"basics/instrumented_testing_basics/#tests-location","text":"Unlike regular jvm tests, instrumented tests located in different src: androidTest","title":"Tests location"},{"location":"basics/instrumented_testing_basics/#why-different-src","text":"You need to write code which describes a communication between your application and android device. For instance, clicking some buttons and checking that particular content has been shown, etc. All that code should be compiled, somehow installed on the device and should make yours application to trigger the checks you need. This code can't be a part of testSrc , because of that case we run it on JVM . There is no information about everything related to android. Every Android SDK class instance used in JVM test will be stubbed. In androidSrc we have a real knowledge about Android SDK","title":"Why different src?"},{"location":"basics/instrumented_testing_basics/#how-tests-run-under-the-hood","text":"To be able to run your tests on CI and make it a part of CD , it's really important to understand how it works under the hood.","title":"How tests run under the hood"},{"location":"basics/instrumented_testing_basics/#21-build","text":"To test our application, we need to build it. We can do that with gradle: # It will build an apk file located in app/build/outputs/debug/debug.apk ./gradlew assembleDebug However, it's not enough for us. Remember? We also need to take care of the code we write in androidTestSrc . It also should be built and will be represented as an apk: # It will build an apk file located in app/build/outputs/debug/debug.apk ./gradlew assembleDebug # It will build an apk file located in app/build/outputs/androidTest/instrumented.apk ./gradlew assembleDebugAndroidTest We've got 2 apks: Application and Test application , which can communicate with Application","title":"2.1 Build"},{"location":"basics/instrumented_testing_basics/#22-install","text":"To do that, we need to use adb adb install debug.apk adb install instrumented.apk","title":"2.2 Install"},{"location":"basics/instrumented_testing_basics/#23-run","text":"For running instrumented tests, AndroidJunitRunner is responsible As an input, you need to provide tests you want to run. As an output, tests result will be provided. All you need to do it's to execute adb command: adb shell am instrument -w -m -e debug false \\ -e class 'com.alexbykov.myapplication.ExampleInstrumentedTest#myTest' \\ com.alexbykov.myapplication.test/androidx.test.runner.AndroidJUnitRunner We need to provide some information about tests needed to be launched: particular class, class with method or package. After execution, you may find junit report in app/build/test-results/ It's also possible to define your own instrumented arguments and get them in tests: //add -e myKey \"test\" to adb command InstrumentationRegistry . getArguments (). getString ( \"myKey\" , \"value\" ) Official documentation","title":"2.3 Run"},{"location":"basics/instrumented_testing_basics/#instrumented-testing-types","text":"UI Screenshot or Snapshot Migration They are not a replacement of each other, they are complement to each other","title":"Instrumented testing types"},{"location":"basics/screenshot_testing/","text":"Screenshot testing What it is Screenshot tests (also called snapshot tests) are a special type of UI tests that inflate a view, take a screenshot of it, and compare it to an already stored image taken as reference. This reference is considered the source of truth: it depicts how the UI must be displayed, pixel by pixel. If the generated snapshot file from the test differs from the reference, the test fails, passes otherwise. The first documented use of screenshot testing dates of April 2011, in a blog post where Diego Torres Milano explains how to do visual image comparison with Monkeyrunner. Nevertheless, it started gaining popularity in Android since 8th October 2015, when Facebook open sourced the first version of their snapshot testing library. Motivation Screenshot tests are easy to write and maintain and run fast (\u2248 1 sec per test), what makes them affordable to execute upon PRs. They focus on detecting visual bugs that standard UiAutomator and Espresso cannot, namely UI defects introduced by updates in libraries like Material design and Constraint Layout Spacing, styles and themes (light mode vs. dark mode) Layout correctness: Multiline text does not cut off No view overlapping on visibility changes Proper view alignment under different screen densities and/or font sizes Text, icons and UI alignment on RTL & LTR languages, among other language related issues How it works First of all, we need to understand that snapshot testing process differs from standard testing. Most snapshot testing frameworks provide two main tasks: Record : Generates a snapshot file. Executed locally. this file will be reviewed by our peers, and once approved, uploaded to the CI as our source of truth. All further verification fo that test will be made against it. Verify : When executed, it generates a snapshot file for each test that will be compared, pixel by pixel, with the one taken as reference when recording. Executed on the CI. Once everybody in the team has agreed the same device setup for snapshot testing, the process goes as follows: Create a test that snapshots a view. This could range from a view dynamically inflated to the activity that is shown on the emulator/device screen. Make sure adb only lists the snapshot testing device/emulator. That is because instrumentation tests run on all emulators/devices adb detects. This might cause some issues like snapshot file duplication errors, longer running times, and the like. Execute \"record\" locally to generate the corresponding snapshot files and open the corresponding PR containing them. Some teammates review the snapshots to confirm that they reflect the state of the view we are expecting. Once approved and merged, this snapshot is taken as source of truth for future verifications of the tests. Verify the snapshots periodically (e.g. on every PR, once a day, etc.). Ideally you have a CI with a job configured to \"verify\" them. This means, it executes the snapshot tests with the most up-to-date code, and takes a screenshot. After that, it compares this screenshot with the last one uploaded to the CI or the one being pushed in the PR. If they both match, the test passes, fails otherwise. Challenges Unfortunately, screenshot tests also come with their own problems. Flakiness : Same as with UI tests, snapshot test face flakiness problems. On one hand, we face similar problems to UI Testing. Most of the issues described in the flakiness section also apply to screenshot tests. Additionally, snapshot test suffer from other issues. Eli Hart from Airbnb has already written an article about most of them, and I can say I've faced even more. I'll state them briefly: Rendering issues : Hardware accelerated drawing model might cause issues like wrongly rendered pixels, especially with Composables. See more here Drawable caching : the Android system maintains a cache of drawables. If the bitmap of such drawable is shared in several locations, the result could vary depending on whether the bitmap has already been cached in a previous test or not. This introduces flakiness in pixel by pixel comparisons. Cached state in Views : some views like ConstraintLayout have their own cache, so that in the next pas they do not have to recalculate anything, what might cause flakiness. Dates : if displaying dates that depend on the current time, the screenshots will change on every rerun and fail while comparing to the one taken as reference. Image loading from urls : some libraries like Picasso and Glide download images from urls asynchronously under the hood. These makes tests nondeterministic, because we cannot ensure the image loading state of the ImageView before taking the screenshot: Random blank image or loading if it was taken before the image was downloaded Error image placeholders if the download failed, etc. RecyclerView prefetching : By default, RecyclerView\u2019s LinearLayoutManager prefetches off screen views outside its viewport while the UI thread is idle between frame. FillViewPort measuring : the size of the fillViewPort view might not have been calculated before taking the screenshot. This might result into that View being cut off, so some of its children would not be visible Animation libraries : libraries like lottie, that load an animation into an ImageView from a Json file. Disabling animations do not take effect on this. They happen asynchronously so we cannot guarantee at which stage is the animation before taking the screenshot. Webview mocking : same problems as asynchronous loading. We cannot ensure the content will be loaded before taking the snapshot We'll describe them in detail in their own section in the future. Emulator configuration in all parts involved ( instrumented snapshot tests only ) Emulators freezing on the CI when idle for a long time. The best practice is the same as with UI tests: close them right after running the tests Synchronizing emulators start up before running tests. Instrumented snapshot tests run on every device adb can detect. For that, we need to start all the emulators and wait till they are ready. We can achieve this by using tools like swarmer . The new official Gradle Managed Virtual Devices looks also promising, although it enables us to run UI tests only for now. Avoid \"Insufficient Storage\" errors by starting adb emulators with the --wipe_data option One could also use real devices for snapshot testing, but it does not scale. Devices are costly, and you need to provide the same model to every single developer in the team, as well as your CI. Important Developers that have integrated UI test previously into their build pipelines, know that instrumented UI tests are rather slow. One popular alternative is to use Robolectric to run UI test on the JVM. This makes UI tests much faster. But this is not the same with snapshot tests. Execution time : Instrumented snapshot tests without interactions can run very fast, in less than 1 second. Test Sharding : JVM snapshot tests can not be sharded. That hinders speed wins by running them in parallel. Therefore, the real power of running snapshot tests on the JVM is not having to deal with emulators. Frameworks Instrumented Facebook Screenshot tests : First screenshot testing framework on Android. Shot : Written on top of Facebook by Pedro. Testify : Shopify library. Dropshots : Dropbox library. Last one open sourced. JVM Paparazzi : Square framework Unlike UI tests, it is not possible to write shared tests Related links Blog posts An introduction to snapshot testing on Android in 2021 The secrets of effectively snapshot testing on Android Code repos Road to Effective Snapshot Testing Conclusion Although it has become very popular recently, screenshot testing has been possible on Android since 2015. Differs from UI testing on its purpose: Screenshot testing focuses on the visuals, how the views are displayed under different configurations. Screenshot tests requires at least 2 tasks, record (locally) and verify (on the CI). The process also requires more steps than standard UI testing. Flakiness is also an issue. Screenshot tests share some common troubles with UI tests, but face many more problems additionally. There are numerous open source frameworks for instrumented screenshot testing, and even one for the JVM. However, no shared tests are possible.","title":"Screenshot testing"},{"location":"basics/screenshot_testing/#screenshot-testing","text":"","title":"Screenshot testing"},{"location":"basics/screenshot_testing/#what-it-is","text":"Screenshot tests (also called snapshot tests) are a special type of UI tests that inflate a view, take a screenshot of it, and compare it to an already stored image taken as reference. This reference is considered the source of truth: it depicts how the UI must be displayed, pixel by pixel. If the generated snapshot file from the test differs from the reference, the test fails, passes otherwise. The first documented use of screenshot testing dates of April 2011, in a blog post where Diego Torres Milano explains how to do visual image comparison with Monkeyrunner. Nevertheless, it started gaining popularity in Android since 8th October 2015, when Facebook open sourced the first version of their snapshot testing library.","title":"What it is"},{"location":"basics/screenshot_testing/#motivation","text":"Screenshot tests are easy to write and maintain and run fast (\u2248 1 sec per test), what makes them affordable to execute upon PRs. They focus on detecting visual bugs that standard UiAutomator and Espresso cannot, namely UI defects introduced by updates in libraries like Material design and Constraint Layout Spacing, styles and themes (light mode vs. dark mode) Layout correctness: Multiline text does not cut off No view overlapping on visibility changes Proper view alignment under different screen densities and/or font sizes Text, icons and UI alignment on RTL & LTR languages, among other language related issues","title":"Motivation"},{"location":"basics/screenshot_testing/#how-it-works","text":"First of all, we need to understand that snapshot testing process differs from standard testing. Most snapshot testing frameworks provide two main tasks: Record : Generates a snapshot file. Executed locally. this file will be reviewed by our peers, and once approved, uploaded to the CI as our source of truth. All further verification fo that test will be made against it. Verify : When executed, it generates a snapshot file for each test that will be compared, pixel by pixel, with the one taken as reference when recording. Executed on the CI. Once everybody in the team has agreed the same device setup for snapshot testing, the process goes as follows: Create a test that snapshots a view. This could range from a view dynamically inflated to the activity that is shown on the emulator/device screen. Make sure adb only lists the snapshot testing device/emulator. That is because instrumentation tests run on all emulators/devices adb detects. This might cause some issues like snapshot file duplication errors, longer running times, and the like. Execute \"record\" locally to generate the corresponding snapshot files and open the corresponding PR containing them. Some teammates review the snapshots to confirm that they reflect the state of the view we are expecting. Once approved and merged, this snapshot is taken as source of truth for future verifications of the tests. Verify the snapshots periodically (e.g. on every PR, once a day, etc.). Ideally you have a CI with a job configured to \"verify\" them. This means, it executes the snapshot tests with the most up-to-date code, and takes a screenshot. After that, it compares this screenshot with the last one uploaded to the CI or the one being pushed in the PR. If they both match, the test passes, fails otherwise.","title":"How it works"},{"location":"basics/screenshot_testing/#challenges","text":"Unfortunately, screenshot tests also come with their own problems. Flakiness : Same as with UI tests, snapshot test face flakiness problems. On one hand, we face similar problems to UI Testing. Most of the issues described in the flakiness section also apply to screenshot tests. Additionally, snapshot test suffer from other issues. Eli Hart from Airbnb has already written an article about most of them, and I can say I've faced even more. I'll state them briefly: Rendering issues : Hardware accelerated drawing model might cause issues like wrongly rendered pixels, especially with Composables. See more here Drawable caching : the Android system maintains a cache of drawables. If the bitmap of such drawable is shared in several locations, the result could vary depending on whether the bitmap has already been cached in a previous test or not. This introduces flakiness in pixel by pixel comparisons. Cached state in Views : some views like ConstraintLayout have their own cache, so that in the next pas they do not have to recalculate anything, what might cause flakiness. Dates : if displaying dates that depend on the current time, the screenshots will change on every rerun and fail while comparing to the one taken as reference. Image loading from urls : some libraries like Picasso and Glide download images from urls asynchronously under the hood. These makes tests nondeterministic, because we cannot ensure the image loading state of the ImageView before taking the screenshot: Random blank image or loading if it was taken before the image was downloaded Error image placeholders if the download failed, etc. RecyclerView prefetching : By default, RecyclerView\u2019s LinearLayoutManager prefetches off screen views outside its viewport while the UI thread is idle between frame. FillViewPort measuring : the size of the fillViewPort view might not have been calculated before taking the screenshot. This might result into that View being cut off, so some of its children would not be visible Animation libraries : libraries like lottie, that load an animation into an ImageView from a Json file. Disabling animations do not take effect on this. They happen asynchronously so we cannot guarantee at which stage is the animation before taking the screenshot. Webview mocking : same problems as asynchronous loading. We cannot ensure the content will be loaded before taking the snapshot We'll describe them in detail in their own section in the future. Emulator configuration in all parts involved ( instrumented snapshot tests only ) Emulators freezing on the CI when idle for a long time. The best practice is the same as with UI tests: close them right after running the tests Synchronizing emulators start up before running tests. Instrumented snapshot tests run on every device adb can detect. For that, we need to start all the emulators and wait till they are ready. We can achieve this by using tools like swarmer . The new official Gradle Managed Virtual Devices looks also promising, although it enables us to run UI tests only for now. Avoid \"Insufficient Storage\" errors by starting adb emulators with the --wipe_data option One could also use real devices for snapshot testing, but it does not scale. Devices are costly, and you need to provide the same model to every single developer in the team, as well as your CI. Important Developers that have integrated UI test previously into their build pipelines, know that instrumented UI tests are rather slow. One popular alternative is to use Robolectric to run UI test on the JVM. This makes UI tests much faster. But this is not the same with snapshot tests. Execution time : Instrumented snapshot tests without interactions can run very fast, in less than 1 second. Test Sharding : JVM snapshot tests can not be sharded. That hinders speed wins by running them in parallel. Therefore, the real power of running snapshot tests on the JVM is not having to deal with emulators.","title":"Challenges"},{"location":"basics/screenshot_testing/#frameworks","text":"Instrumented Facebook Screenshot tests : First screenshot testing framework on Android. Shot : Written on top of Facebook by Pedro. Testify : Shopify library. Dropshots : Dropbox library. Last one open sourced. JVM Paparazzi : Square framework Unlike UI tests, it is not possible to write shared tests","title":"Frameworks"},{"location":"basics/screenshot_testing/#related-links","text":"","title":"Related links"},{"location":"basics/screenshot_testing/#blog-posts","text":"An introduction to snapshot testing on Android in 2021 The secrets of effectively snapshot testing on Android","title":"Blog posts"},{"location":"basics/screenshot_testing/#code-repos","text":"Road to Effective Snapshot Testing","title":"Code repos"},{"location":"basics/screenshot_testing/#conclusion","text":"Although it has become very popular recently, screenshot testing has been possible on Android since 2015. Differs from UI testing on its purpose: Screenshot testing focuses on the visuals, how the views are displayed under different configurations. Screenshot tests requires at least 2 tasks, record (locally) and verify (on the CI). The process also requires more steps than standard UI testing. Flakiness is also an issue. Screenshot tests share some common troubles with UI tests, but face many more problems additionally. There are numerous open source frameworks for instrumented screenshot testing, and even one for the JVM. However, no shared tests are possible.","title":"Conclusion"},{"location":"basics/testing_theory/","text":"Testing theory This article will cover the most primitive things that will allow you to understand better the terms and general concepts regarding testing. It is also worth noting that there will be an accent on UI tests, because we are in CookBook for UI tests. Where it all starts Why do I need testing? Users of your application/system/whatever should receive a quality product. Thus, testing is about ensuring the quality of the product. Testing also includes creating a test plan, creating/conducting tests themselves and analyzing results of testing. There are different means to classify tests. We will focus on the following Granularity Implementation details known Running environment Purpose Classifying tests by granularity To begin with, let's see the testing pyramid. Testing Pyramid (source Martin Fowler, triangle authorship Kent C. Dotts) End-to-End tests (E2E) : these tests verify that all components in every single layer of the application work together as expected. They allow to test User stories - how our users are going to use the application. This makes them incredibly important both for business and for an ordinary developer or tester. However, this is the most unstable type of test, and the cost of creating and maintaining them is high. That's because there are a lot of factors we cannot control (e.g. network failures, clean user state before and after the test, etc.). Integration tests : they focus on validating the interaction of 2 (or more) entities at once at the same time, but not the full system as with E2E tests. Most components involved are not mocks or stubs, but doubles or fakes. Unit tests : they only cover the smallest testable unit, usually a function, or view. The width of each block is actually the ratio of the number of different types of tests to each other. For example, a lot of Unit tests is usually considered correct in the pyramid, but much less E2E tests. This is due to two key parameters - stability and the cost of supporting each type of testing. Unit tests have the highest stability, they are the fastest and they have the lowest cost of support. However, Unit tests do not intend to verify User stories e.g. the login flow of your application (which for example contains 5 screens). Classifying tests by the implementation details known Testing can be classified regarding their access to the implementation details as follows: White-box testing : is a type of testing in which we have full access to the implementation and can interact with it. We know which output data will be with given input data. Black-box testing : is a type of testing in which we don't have access to the implementation and cannot interact with it, however, we know which output data should be with given input data. Gray-box testing : is a type of testing when you have partial access to the implementation (for example, not to all entities that being tested). At the same time, we know what output data will be with given input data. Most of the instrumentation test frameworks like Espresso and UiAutomator use Gray-box testing: They use view ids or text to interact with the Views. The Robot and Page Object pattern try to help with this by adding another abstraction layer that segregates the WHAT from the HOW . You can find more on that in the Page Object section Apart from these 2 clusters, there are two other interesting means to classify tests, as pictured below Disclaimer Important to note that these classifications are not mutually exclusive. One can write, for instance Ui tests that run on the JVM and only test one view (i.e. unit test) Non-Ui tests that run on a device (i.e. instrumented) and test several components (i.e. integration test) Classifying tests by the environment where they can run on Depending on the environment the tests can run on, we get the following distribution JVM (Java Virtual Machine) tests : can run without the need of an emulator or physical device. These tests do not contain Android-specific code, although they might mock it under the hood. Instrumentation tests : those that require an emulator or physical device. These are tests that contain Android-specific code, like Views. Such code requires a DVM (Dalvik Virtual Machine) or Android Runtime (ART) since 5.0 to be executed, and therefore it cannot run on the JVM but on a device. Shared tests : These are tests that are written once, but can run either as JVM or as Instrumentation tests. The principle is simple: Write once, run everywhere . These tests contain Android-specific code. Such code would need a device to run on. In order to enable that, when running on the JVM, that code is replaced by mocks under the hood. Classifying tests by their purpose Depending on the goal of our tests, they are split into the following categories UI tests : focus on testing the WHAT (interaction/navigation); WHAT is displayed when interacting with the screen elements. Screenshot/Snapshot tests : focus on testing the HOW (visuals): HOW a view is displayed under a given state or configuration. Non-UI tests : focus on testing non-ui related code like BUSINESS LOGIC or DATABASE MIGRATIONS among others Manual testing Although we've mainly focused on automated tests, it is also worth noting the importance of manual testing in the quality of an app. In manual testing se do not write automated tests, but rather steps we need to reproduce the User story or bug. This is especially needed for User stories that are very hard or impossible to completely automatize, or just not worth the effort. In this group are also very edge cases that would require extremely complex and large tests, if even possible. That is where manual testing comes in. Most E2E tests are performed manually. Automated E2E tests are the hardest to get stable, due to various components being involved, some of them out of our control (e.g. stable internet connectivity). Hence manual testing has an important role and cannot be completely replaced by conventional types of testing. But don't forget that this is essentially a question of the acceptable quality of a particular product. Conclusion We understood that we can classify tests by different criteria, and every test belongs to one group in every classification E2E tests are the most unstable, the longest and the most expensive in terms of support but they allow you to test entire User stories and can do it even on every merge. We got the basic concepts of testing, learned about the difference between Black/White/Gray-box testing. We are aware that, although Snapshot tests and UI tests verify the Ui, they serve different purposes. We got that we can write Shared tests that can run on the JVM or on the device. We understood that automated tests cannot completely replace manual testing.","title":"Testing theory"},{"location":"basics/testing_theory/#testing-theory","text":"This article will cover the most primitive things that will allow you to understand better the terms and general concepts regarding testing. It is also worth noting that there will be an accent on UI tests, because we are in CookBook for UI tests.","title":"Testing theory"},{"location":"basics/testing_theory/#where-it-all-starts","text":"Why do I need testing? Users of your application/system/whatever should receive a quality product. Thus, testing is about ensuring the quality of the product. Testing also includes creating a test plan, creating/conducting tests themselves and analyzing results of testing. There are different means to classify tests. We will focus on the following Granularity Implementation details known Running environment Purpose","title":"Where it all starts"},{"location":"basics/testing_theory/#classifying-tests-by-granularity","text":"To begin with, let's see the testing pyramid. Testing Pyramid (source Martin Fowler, triangle authorship Kent C. Dotts) End-to-End tests (E2E) : these tests verify that all components in every single layer of the application work together as expected. They allow to test User stories - how our users are going to use the application. This makes them incredibly important both for business and for an ordinary developer or tester. However, this is the most unstable type of test, and the cost of creating and maintaining them is high. That's because there are a lot of factors we cannot control (e.g. network failures, clean user state before and after the test, etc.). Integration tests : they focus on validating the interaction of 2 (or more) entities at once at the same time, but not the full system as with E2E tests. Most components involved are not mocks or stubs, but doubles or fakes. Unit tests : they only cover the smallest testable unit, usually a function, or view. The width of each block is actually the ratio of the number of different types of tests to each other. For example, a lot of Unit tests is usually considered correct in the pyramid, but much less E2E tests. This is due to two key parameters - stability and the cost of supporting each type of testing. Unit tests have the highest stability, they are the fastest and they have the lowest cost of support. However, Unit tests do not intend to verify User stories e.g. the login flow of your application (which for example contains 5 screens).","title":"Classifying tests by granularity"},{"location":"basics/testing_theory/#classifying-tests-by-the-implementation-details-known","text":"Testing can be classified regarding their access to the implementation details as follows: White-box testing : is a type of testing in which we have full access to the implementation and can interact with it. We know which output data will be with given input data. Black-box testing : is a type of testing in which we don't have access to the implementation and cannot interact with it, however, we know which output data should be with given input data. Gray-box testing : is a type of testing when you have partial access to the implementation (for example, not to all entities that being tested). At the same time, we know what output data will be with given input data. Most of the instrumentation test frameworks like Espresso and UiAutomator use Gray-box testing: They use view ids or text to interact with the Views. The Robot and Page Object pattern try to help with this by adding another abstraction layer that segregates the WHAT from the HOW . You can find more on that in the Page Object section Apart from these 2 clusters, there are two other interesting means to classify tests, as pictured below Disclaimer Important to note that these classifications are not mutually exclusive. One can write, for instance Ui tests that run on the JVM and only test one view (i.e. unit test) Non-Ui tests that run on a device (i.e. instrumented) and test several components (i.e. integration test)","title":"Classifying tests by the implementation details known"},{"location":"basics/testing_theory/#classifying-tests-by-the-environment-where-they-can-run-on","text":"Depending on the environment the tests can run on, we get the following distribution JVM (Java Virtual Machine) tests : can run without the need of an emulator or physical device. These tests do not contain Android-specific code, although they might mock it under the hood. Instrumentation tests : those that require an emulator or physical device. These are tests that contain Android-specific code, like Views. Such code requires a DVM (Dalvik Virtual Machine) or Android Runtime (ART) since 5.0 to be executed, and therefore it cannot run on the JVM but on a device. Shared tests : These are tests that are written once, but can run either as JVM or as Instrumentation tests. The principle is simple: Write once, run everywhere . These tests contain Android-specific code. Such code would need a device to run on. In order to enable that, when running on the JVM, that code is replaced by mocks under the hood.","title":"Classifying tests by the environment where they can run on"},{"location":"basics/testing_theory/#classifying-tests-by-their-purpose","text":"Depending on the goal of our tests, they are split into the following categories UI tests : focus on testing the WHAT (interaction/navigation); WHAT is displayed when interacting with the screen elements. Screenshot/Snapshot tests : focus on testing the HOW (visuals): HOW a view is displayed under a given state or configuration. Non-UI tests : focus on testing non-ui related code like BUSINESS LOGIC or DATABASE MIGRATIONS among others","title":"Classifying tests by their purpose"},{"location":"basics/testing_theory/#manual-testing","text":"Although we've mainly focused on automated tests, it is also worth noting the importance of manual testing in the quality of an app. In manual testing se do not write automated tests, but rather steps we need to reproduce the User story or bug. This is especially needed for User stories that are very hard or impossible to completely automatize, or just not worth the effort. In this group are also very edge cases that would require extremely complex and large tests, if even possible. That is where manual testing comes in. Most E2E tests are performed manually. Automated E2E tests are the hardest to get stable, due to various components being involved, some of them out of our control (e.g. stable internet connectivity). Hence manual testing has an important role and cannot be completely replaced by conventional types of testing. But don't forget that this is essentially a question of the acceptable quality of a particular product.","title":"Manual testing"},{"location":"basics/testing_theory/#conclusion","text":"We understood that we can classify tests by different criteria, and every test belongs to one group in every classification E2E tests are the most unstable, the longest and the most expensive in terms of support but they allow you to test entire User stories and can do it even on every merge. We got the basic concepts of testing, learned about the difference between Black/White/Gray-box testing. We are aware that, although Snapshot tests and UI tests verify the Ui, they serve different purposes. We got that we can write Shared tests that can run on the JVM or on the device. We understood that automated tests cannot completely replace manual testing.","title":"Conclusion"},{"location":"basics/ui_testing/","text":"UI testing UI tests are a part of instrumentation tests. That's why everything from Instrumented tests topic is applicable to UI testing. Ui testing goal is to check some scenarios of using your application. Usually, we do ui testing for some common users scenario: Login to your application Process of ordering Creating new chat And so on Simple Ui test can't catch the problem with wrong padding or wrong color of your view (only in cases where you specially check that) - because there is another type of tests used to catch such problems called Screenshot testing - you can read Screenshot testing article to get familiar with that. So, if once we decided that we need to check users scenarios - we should definitely start writing Ui tests and let's start with tools, that will help us to write this type of tests. Main tool - Espresso and UiAutomator Nowadays there is no doubt - for native UI testing on Android we should use Espresso for testing your application. This is a main tool that allow us to make every test possible to access your codebase. That means tests with Espresso allow us to write white-box tests. But you still able to write black-box tests or even gray-box with Espresso. Why Espresso is so cool? It is synchronized with the main thread of your app and performs action only when main thread is in idle state. It makes Espresso extremely reliable and stable (than any other existing tool) for Ui testing on Android. Keep in mind that everything that isn't related to your app (for example, permission dialogs, volume +/- buttons, home button and etc) - can't be accessed directly within Espresso. But don't worry - you can use UiAutomator for that purpose, and it can be called directly from test, written in Espresso. UiAutomator is a slower tool and can support tests only in black-box style, but it can test everything outside your application on your phone. Many cross-platforms tools for testing using UiAutomator . These two tools are the main tools that everyone using for Ui testing on Android. Remember, it's possible to create tests that will use both frameworks. For example, you can find your views and interact with them by Espresso and in the same test call UiAutomator to perform something specific, like pressing volume button. But for usual testing you should prefer Espresso rather then UiAutomator . Espresso as well as the Jetpack Compose Testing Library support Robolectric, while UiAutomator doesn't. That means, one can write Shared Tests written in Espresso or with Compose Library. Espresso \u2795 Access to codebase \u2795 Very fast and more stable than UiAutomator \u2796 Can't perform OS actions \u2796 Can't do anything outside your application UiAutomator \u2795 Can do action outside your application \u2796 Much fewer abilities to find the views and interact with them \u2796 Slower and less stable than Espresso \u2796 Nowadays it shouldn't be a problem but still, it's requires minSdk >= Android 4.3 (API level 18) You can read more about both frameworks at official documentation. Espresso UiAutomator Writing test on Espresso Let's get familiar with Espresso. Basically to start writing Ui tests we should: Find the View (using ViewMatchers ) Interact with that View (using ViewInteraction ) Check state of View (using ViewAssertion ) Let's start with the basic test. We have activity with one button (and no text shown), once we press it - text \"Button pressed\" shown. 1. We should specify which activity we should run. import androidx.test.ext.junit.rules.ActivityScenarioRule import org.junit.Rule ... @get : Rule var activityRule : ActivityScenarioRule < MainActivity > = ActivityScenarioRule ( MainActivity :: class . java ) 2. Create test and find our button (with onView() method and withId assertion by id) @Test fun pressButtonAndCheckText () { onView ( withId ( R . id . button )) } Disclaimer Important addition - when you are trying to find some View that, for example, has same id like another View on that screen (that situation usually happens when you are trying to test screens with RecyclerView ) you can add more Matchers Simply place all Matchers that apply to single View on your screen inside of allOf() method It will look like that @Test fun someTest() { onView(allOf(withId(R.id.button), withText(\"Click me\"))) } 3.Then we should perform click on it (with click() method from ViewInteraction ) @Test fun pressButtonAndCheckText () { onView ( withId ( R . id . button )) . perform ( click ()) } 4. And finally, let's check that our text is shown (find the TextView and assert that it is displayed) @Test fun pressButtonAndCheckText () { ... onView ( withId ( R . id . textview )) . check ( matches ( isDisplayed ())) } Once we run it (run it as usual test, but you should connect real phone or run emulator), we will see standard test result Final code of our test @RunWith ( AndroidJUnit4 :: class ) class ExampleInstrumentedTest { @get : Rule var activityRule : ActivityScenarioRule < MainActivity > = ActivityScenarioRule ( MainActivity :: class . java ) @Test fun pressButtonAndCheckText () { onView ( withId ( R . id . button )) . perform ( click ()) onView ( withId ( R . id . textview )) . check ( matches ( isDisplayed ())) } } Espresso Cheat Sheet Espresso is quite powerful tool with a lot of abstractions inside it. But there is a quite famous \"cheat-sheet\" for Espresso, I hope it will be helpful for everybody. From that cheat-sheet you can understand which methods you can use for finding the Views , which methods for interactions with Views and which methods for checking the data. Something like conclusion To be honest, there is a lot of edge-cases once you are trying to write your own tests. That's one of the reasons why that CookBook was created, so we are suggesting to read next articles to understand how to overcome many of common problems that you will face in Ui testing. Beside that fact, that Espresso and UiAutomator are the main tools to do Ui testing on Android, you may notice at Companies experience article that almost nobody uses only these tools. That happens because there a lot of solutions nowadays created over Espresso and UiAutomator (that means that they use these frameworks under the hood), that makes your test even more stable and readable. Most companies do not use Espresso or UiAutomator directly, but through frameworks that wrap them under the hood: Espresso Kakao Barista Kaspresso UiAutomator Kaspresso (Kautomator) Compose Test Library Kaspresso (since 1.4.0) For example, you can see at Page object article how people overcome problem of readability of your tests and usually prefer something like Kakao library . Or take a look at Flakiness article where you can find information why even in Espresso our tests might be failed even if everything looks ok and test can pass from second attempt. That article will tell you which tools helps us to minimize that risks.","title":"UI testing"},{"location":"basics/ui_testing/#ui-testing","text":"UI tests are a part of instrumentation tests. That's why everything from Instrumented tests topic is applicable to UI testing. Ui testing goal is to check some scenarios of using your application. Usually, we do ui testing for some common users scenario: Login to your application Process of ordering Creating new chat And so on Simple Ui test can't catch the problem with wrong padding or wrong color of your view (only in cases where you specially check that) - because there is another type of tests used to catch such problems called Screenshot testing - you can read Screenshot testing article to get familiar with that. So, if once we decided that we need to check users scenarios - we should definitely start writing Ui tests and let's start with tools, that will help us to write this type of tests.","title":"UI testing"},{"location":"basics/ui_testing/#main-tool-espresso-and-uiautomator","text":"Nowadays there is no doubt - for native UI testing on Android we should use Espresso for testing your application. This is a main tool that allow us to make every test possible to access your codebase. That means tests with Espresso allow us to write white-box tests. But you still able to write black-box tests or even gray-box with Espresso. Why Espresso is so cool? It is synchronized with the main thread of your app and performs action only when main thread is in idle state. It makes Espresso extremely reliable and stable (than any other existing tool) for Ui testing on Android. Keep in mind that everything that isn't related to your app (for example, permission dialogs, volume +/- buttons, home button and etc) - can't be accessed directly within Espresso. But don't worry - you can use UiAutomator for that purpose, and it can be called directly from test, written in Espresso. UiAutomator is a slower tool and can support tests only in black-box style, but it can test everything outside your application on your phone. Many cross-platforms tools for testing using UiAutomator . These two tools are the main tools that everyone using for Ui testing on Android. Remember, it's possible to create tests that will use both frameworks. For example, you can find your views and interact with them by Espresso and in the same test call UiAutomator to perform something specific, like pressing volume button. But for usual testing you should prefer Espresso rather then UiAutomator . Espresso as well as the Jetpack Compose Testing Library support Robolectric, while UiAutomator doesn't. That means, one can write Shared Tests written in Espresso or with Compose Library.","title":"Main tool - Espresso and UiAutomator"},{"location":"basics/ui_testing/#espresso","text":"\u2795 Access to codebase \u2795 Very fast and more stable than UiAutomator \u2796 Can't perform OS actions \u2796 Can't do anything outside your application","title":"Espresso"},{"location":"basics/ui_testing/#uiautomator","text":"\u2795 Can do action outside your application \u2796 Much fewer abilities to find the views and interact with them \u2796 Slower and less stable than Espresso \u2796 Nowadays it shouldn't be a problem but still, it's requires minSdk >= Android 4.3 (API level 18) You can read more about both frameworks at official documentation. Espresso UiAutomator","title":"UiAutomator"},{"location":"basics/ui_testing/#writing-test-on-espresso","text":"Let's get familiar with Espresso. Basically to start writing Ui tests we should: Find the View (using ViewMatchers ) Interact with that View (using ViewInteraction ) Check state of View (using ViewAssertion ) Let's start with the basic test. We have activity with one button (and no text shown), once we press it - text \"Button pressed\" shown. 1. We should specify which activity we should run. import androidx.test.ext.junit.rules.ActivityScenarioRule import org.junit.Rule ... @get : Rule var activityRule : ActivityScenarioRule < MainActivity > = ActivityScenarioRule ( MainActivity :: class . java ) 2. Create test and find our button (with onView() method and withId assertion by id) @Test fun pressButtonAndCheckText () { onView ( withId ( R . id . button )) } Disclaimer Important addition - when you are trying to find some View that, for example, has same id like another View on that screen (that situation usually happens when you are trying to test screens with RecyclerView ) you can add more Matchers Simply place all Matchers that apply to single View on your screen inside of allOf() method It will look like that @Test fun someTest() { onView(allOf(withId(R.id.button), withText(\"Click me\"))) } 3.Then we should perform click on it (with click() method from ViewInteraction ) @Test fun pressButtonAndCheckText () { onView ( withId ( R . id . button )) . perform ( click ()) } 4. And finally, let's check that our text is shown (find the TextView and assert that it is displayed) @Test fun pressButtonAndCheckText () { ... onView ( withId ( R . id . textview )) . check ( matches ( isDisplayed ())) } Once we run it (run it as usual test, but you should connect real phone or run emulator), we will see standard test result Final code of our test @RunWith ( AndroidJUnit4 :: class ) class ExampleInstrumentedTest { @get : Rule var activityRule : ActivityScenarioRule < MainActivity > = ActivityScenarioRule ( MainActivity :: class . java ) @Test fun pressButtonAndCheckText () { onView ( withId ( R . id . button )) . perform ( click ()) onView ( withId ( R . id . textview )) . check ( matches ( isDisplayed ())) } }","title":"Writing test on Espresso"},{"location":"basics/ui_testing/#espresso-cheat-sheet","text":"Espresso is quite powerful tool with a lot of abstractions inside it. But there is a quite famous \"cheat-sheet\" for Espresso, I hope it will be helpful for everybody. From that cheat-sheet you can understand which methods you can use for finding the Views , which methods for interactions with Views and which methods for checking the data.","title":"Espresso Cheat Sheet"},{"location":"basics/ui_testing/#something-like-conclusion","text":"To be honest, there is a lot of edge-cases once you are trying to write your own tests. That's one of the reasons why that CookBook was created, so we are suggesting to read next articles to understand how to overcome many of common problems that you will face in Ui testing. Beside that fact, that Espresso and UiAutomator are the main tools to do Ui testing on Android, you may notice at Companies experience article that almost nobody uses only these tools. That happens because there a lot of solutions nowadays created over Espresso and UiAutomator (that means that they use these frameworks under the hood), that makes your test even more stable and readable. Most companies do not use Espresso or UiAutomator directly, but through frameworks that wrap them under the hood: Espresso Kakao Barista Kaspresso UiAutomator Kaspresso (Kautomator) Compose Test Library Kaspresso (since 1.4.0) For example, you can see at Page object article how people overcome problem of readability of your tests and usually prefer something like Kakao library . Or take a look at Flakiness article where you can find information why even in Espresso our tests might be failed even if everything looks ok and test can pass from second attempt. That article will tell you which tools helps us to minimize that risks.","title":"Something like conclusion"},{"location":"basics/ui_tests_vs_snapshot_tests/","text":"It might be confusing to understand when to write UI test rather than Screenshot tests and vice versa. They do not replace each other. Their focus is different as previously mentioned. So let's imagine the following screen, which is a RecyclerView What to UI test A UI test would verify , e.g. that after deleting a row in the RecyclerView, that row is not displayed anymore. It would test WHAT is displayed after interacting with the view Therefore, write a Ui test if: You need to interact with one or more views You need to assert a certain behaviour after such interactions Navigation to another screen Visibility of some UI elements You do not mind how pixel perfect every single UI element looks on the screen. You just care about the result of your interactions: WHAT is displayed instead of HOW it is displayed What to Screenshot test On the other hand, a snapshot test would verify HOW that row is displayed under numerous states and configurations: e.g. dark/light mode, LTR/RTL languages, different font sizes, wide/narrow screens... Therefore, write a Snapshot test if: you've made a visual change in an UI element you want to verify HOW that change is displayed under different configurations In this case you are saving time to yourself and everybody involved in the QA process: nobody needs to play around with numerous settings/states to ensure everything looks pixel perfect. That process is cumbersome and you've automated it. Up: Row when system font size set to huge Down: Row in dark mode Use the right tool for the job If you are new to Screenshot testing, don't fall into the trap of thinking that it can replace UI testing. For our test case, we wanted to verify that after deleting a row in the RecyclerView, that row is not displayed anymore. If you had the test already written, it'd be as simple as replacing your view visibility assert with a snapshot one at the end of the test. However, keep in mind that this approach does not solve some common problems with UI testing: Flakiness : Screenshot tests also come with flakiness, and even its own issues e.g. mind dates if displaying any (more about this later). As with UI tests, those problems can be mitigated though. Speed : Fake Snapshot tests : Writing Screenshot tests that interact with views the same way as Ui tests, do not make them any faster . For that you need to write Screenshot tests that just inflate a view under a given state and snapshot it. This is what I call a fake Screenshot test: a Ui test disguised with a snapshot assert . Less-scalable test sharding : If you are using a cloud device service like Firebase test lab with test sharding to speed up the execution, it is not that simple. Snapshot file comparisons are done pixel by pixel. This means, all tests must run on the same device models across all parts involved (devs and CI) to ensure that the resolution, screen size and api create screenshots with identical pixels. This restricts a lot the speed wins of test sharding usually gained with such services. While all UI tests can be distributed among all devices, snapshot tests can only use a portion: those devices with the same config that developers use to record the snapshots. This is depicted below Disclaimer Test sharding allows to evenly split up the test suite into all connected devices. This enables to parallelize your tests. So if you have 300 tests and 30 devices, 10 tests run on every device in parallel, resulting in considerably lower test execution times. Additionally, you will face the following new issues: If still no Snapshot tests in place & planning to run Snapshot tests on emulators , dealing with them does not make things easier ensuring every part involved (i.e. developers and CI) has the same emulator config: snapshot assertions happen pixel by pixel. rendering issues due to hardware acceleration (also on physical devices) insufficient storage errors due to the space taken by the generated files, etc. Actually, if you were using physical devices on the CI and move to emulators due to Snapshot testing, you'll additionally face the following troubles: freezing emulators on the CI synchronizing emulators start up before running the tests Tests become brittle : they fail badly if altered in a seemingly minor way. Let me explain the last point with an example. Remember: we wanted to verify that after deleting a row in the RecyclerView, that row is not displayed anymore. We've written a Snapshot test for that. Imagine you change how the row is displayed. If you verify the snapshot test, it will fail. You need to record a new snapshot including those changes on the row. The issue here is that the focus of your test was to verify that the deleted row is not displayed anymore. What does it have to do with changing the appearance of the row? You guessed it. Nothing. But the test fails because of that. You do not want your test to fail for the wrong reason. You want them to be meaningful. You want them to have a purpose I'd like to mention that you do not even need to change the appearance of the row intentionally. It's enough that your screen displays dates depending on the current time: the date changes on every run, making your \"fake screenshot test\" fail. Again, failing for the wrong reason. Therefore, every subtle change on the screen will require to record a new snapshot, although that change had nothing to do with the initial intention of the test . On the other hand, an UI test would have not failed since we would be asserting whether the deleted row was displayed or not. No visuals involved. Conclusion Use both UI test and Snapshot tests , they complement each other. They aim to assert different things. Avoid fake snapshot tests , they usually add up troubles compared to UI tests rather than mitigating their issues. Use the right tool for the job : using Screenshot tests for testing interactions leads to brittle tests . Further reading Blog post A more detailed blog post on this matter: UI tests vs. snapshot tests on Android: which one should I write? \ud83e\udd14","title":"Ui testing or Screenshot testing?"},{"location":"basics/ui_tests_vs_snapshot_tests/#what-to-ui-test","text":"A UI test would verify , e.g. that after deleting a row in the RecyclerView, that row is not displayed anymore. It would test WHAT is displayed after interacting with the view Therefore, write a Ui test if: You need to interact with one or more views You need to assert a certain behaviour after such interactions Navigation to another screen Visibility of some UI elements You do not mind how pixel perfect every single UI element looks on the screen. You just care about the result of your interactions: WHAT is displayed instead of HOW it is displayed","title":"What to UI test"},{"location":"basics/ui_tests_vs_snapshot_tests/#what-to-screenshot-test","text":"On the other hand, a snapshot test would verify HOW that row is displayed under numerous states and configurations: e.g. dark/light mode, LTR/RTL languages, different font sizes, wide/narrow screens... Therefore, write a Snapshot test if: you've made a visual change in an UI element you want to verify HOW that change is displayed under different configurations In this case you are saving time to yourself and everybody involved in the QA process: nobody needs to play around with numerous settings/states to ensure everything looks pixel perfect. That process is cumbersome and you've automated it. Up: Row when system font size set to huge Down: Row in dark mode","title":"What to Screenshot test"},{"location":"basics/ui_tests_vs_snapshot_tests/#use-the-right-tool-for-the-job","text":"If you are new to Screenshot testing, don't fall into the trap of thinking that it can replace UI testing. For our test case, we wanted to verify that after deleting a row in the RecyclerView, that row is not displayed anymore. If you had the test already written, it'd be as simple as replacing your view visibility assert with a snapshot one at the end of the test. However, keep in mind that this approach does not solve some common problems with UI testing: Flakiness : Screenshot tests also come with flakiness, and even its own issues e.g. mind dates if displaying any (more about this later). As with UI tests, those problems can be mitigated though. Speed : Fake Snapshot tests : Writing Screenshot tests that interact with views the same way as Ui tests, do not make them any faster . For that you need to write Screenshot tests that just inflate a view under a given state and snapshot it. This is what I call a fake Screenshot test: a Ui test disguised with a snapshot assert . Less-scalable test sharding : If you are using a cloud device service like Firebase test lab with test sharding to speed up the execution, it is not that simple. Snapshot file comparisons are done pixel by pixel. This means, all tests must run on the same device models across all parts involved (devs and CI) to ensure that the resolution, screen size and api create screenshots with identical pixels. This restricts a lot the speed wins of test sharding usually gained with such services. While all UI tests can be distributed among all devices, snapshot tests can only use a portion: those devices with the same config that developers use to record the snapshots. This is depicted below Disclaimer Test sharding allows to evenly split up the test suite into all connected devices. This enables to parallelize your tests. So if you have 300 tests and 30 devices, 10 tests run on every device in parallel, resulting in considerably lower test execution times. Additionally, you will face the following new issues: If still no Snapshot tests in place & planning to run Snapshot tests on emulators , dealing with them does not make things easier ensuring every part involved (i.e. developers and CI) has the same emulator config: snapshot assertions happen pixel by pixel. rendering issues due to hardware acceleration (also on physical devices) insufficient storage errors due to the space taken by the generated files, etc. Actually, if you were using physical devices on the CI and move to emulators due to Snapshot testing, you'll additionally face the following troubles: freezing emulators on the CI synchronizing emulators start up before running the tests Tests become brittle : they fail badly if altered in a seemingly minor way. Let me explain the last point with an example. Remember: we wanted to verify that after deleting a row in the RecyclerView, that row is not displayed anymore. We've written a Snapshot test for that. Imagine you change how the row is displayed. If you verify the snapshot test, it will fail. You need to record a new snapshot including those changes on the row. The issue here is that the focus of your test was to verify that the deleted row is not displayed anymore. What does it have to do with changing the appearance of the row? You guessed it. Nothing. But the test fails because of that. You do not want your test to fail for the wrong reason. You want them to be meaningful. You want them to have a purpose I'd like to mention that you do not even need to change the appearance of the row intentionally. It's enough that your screen displays dates depending on the current time: the date changes on every run, making your \"fake screenshot test\" fail. Again, failing for the wrong reason. Therefore, every subtle change on the screen will require to record a new snapshot, although that change had nothing to do with the initial intention of the test . On the other hand, an UI test would have not failed since we would be asserting whether the deleted row was displayed or not. No visuals involved.","title":"Use the right tool for the job"},{"location":"basics/ui_tests_vs_snapshot_tests/#conclusion","text":"Use both UI test and Snapshot tests , they complement each other. They aim to assert different things. Avoid fake snapshot tests , they usually add up troubles compared to UI tests rather than mitigating their issues. Use the right tool for the job : using Screenshot tests for testing interactions leads to brittle tests .","title":"Conclusion"},{"location":"basics/ui_tests_vs_snapshot_tests/#further-reading","text":"","title":"Further reading"},{"location":"basics/ui_tests_vs_snapshot_tests/#blog-post","text":"A more detailed blog post on this matter: UI tests vs. snapshot tests on Android: which one should I write? \ud83e\udd14","title":"Blog post"},{"location":"home/","text":"Home Despite the annual improvement of tools \u2014 everything related to Android instrumented testing still can be challenging and requires a lot of attention from engineers. The goal of this blog is to make the process of introducing instrumented testing into your team smoother and avoid repeating our mistakes. You know nothing about UI & Screenshot Testing and doing your first steps? Chapter Basics will help you You are in a process of adoption of UI & Screenshot Testing? Chapter Practices will answer on the most of questions you probably have Want to adopt UI & Screenshot testing, but don't know where to start from? Chapter Adoption will be helpful Want to say thank you? Star this repository on the Github","title":"Home"},{"location":"home/#home","text":"Despite the annual improvement of tools \u2014 everything related to Android instrumented testing still can be challenging and requires a lot of attention from engineers. The goal of this blog is to make the process of introducing instrumented testing into your team smoother and avoid repeating our mistakes. You know nothing about UI & Screenshot Testing and doing your first steps? Chapter Basics will help you You are in a process of adoption of UI & Screenshot Testing? Chapter Practices will answer on the most of questions you probably have Want to adopt UI & Screenshot testing, but don't know where to start from? Chapter Adoption will be helpful Want to say thank you? Star this repository on the Github","title":"Home"},{"location":"home/contribution/","text":"Contribution If you want to help us to make everything related to UI testing easier to others, you can contact us directly. Alexey Bykov Dmitriy Movchan Sergio Sastre Companies experience template Please, use this template to share your tooling with others ## Company name `UI testing` : `Write:` [ Framework ]( Link to the github/documantation ) <br> `Who:` [Android Engineers, QA Automation]<br> `Runner:` [ Runner ]( Link to the github/documantation ) <br> `Where:` [ Real Device, Emulator ]( Some link if presented ) <br> `How often:` Each commit, Periodically, Before the release, etc <br> `Network:` [ Library ]( Link ) <br> `Test report:` [ Name ]( Link )<br> : `Other` : Include this section if you want to share something useful only `Snapshot testing` : `Tools:` [ Screenshot tests for Android ]( https://github.com/facebook/screenshot-tests-for-android ) <br> `How often to run:` Each commit, Periodically, etc : `Other:` Include this section if you want to share something useful only","title":"Contribution"},{"location":"home/contribution/#contribution","text":"If you want to help us to make everything related to UI testing easier to others, you can contact us directly. Alexey Bykov Dmitriy Movchan Sergio Sastre","title":"Contribution"},{"location":"home/contribution/#companies-experience-template","text":"Please, use this template to share your tooling with others ## Company name `UI testing` : `Write:` [ Framework ]( Link to the github/documantation ) <br> `Who:` [Android Engineers, QA Automation]<br> `Runner:` [ Runner ]( Link to the github/documantation ) <br> `Where:` [ Real Device, Emulator ]( Some link if presented ) <br> `How often:` Each commit, Periodically, Before the release, etc <br> `Network:` [ Library ]( Link ) <br> `Test report:` [ Name ]( Link )<br> : `Other` : Include this section if you want to share something useful only `Snapshot testing` : `Tools:` [ Screenshot tests for Android ]( https://github.com/facebook/screenshot-tests-for-android ) <br> `How often to run:` Each commit, Periodically, etc : `Other:` Include this section if you want to share something useful only","title":"Companies experience template"},{"location":"home/roadmap/","text":"Roadmap Adoption Where it all starts Practises Code style Run UI tests on obfuscated build Shared UI tests guide Test reports","title":"Roadmap"},{"location":"home/roadmap/#roadmap","text":"","title":"Roadmap"},{"location":"home/roadmap/#adoption","text":"Where it all starts","title":"Adoption"},{"location":"home/roadmap/#practises","text":"Code style Run UI tests on obfuscated build Shared UI tests guide Test reports","title":"Practises"},{"location":"practices/emulator_setup/","text":"Emulator setup Basically we have next choices: Manage devices automatically by avd Manage docker containers with emulators by docker Using docker image is the easiest way, however it's important to understand how docker creates device for you. Creating an emulator Before starting to read this topic, make sure you've read an an official documentation Firstly, you need to create an ini configuration: PlayStore.enabled = false abi.type = x86_64 avd.ini.encoding = UTF-8 hw.cpu.arch = x86_64 hw.cpu.ncore = 2 hw.ramSize = 2048 hw.lcd.density = 120 hw.lcd.width = 320 hw.lcd.height = 480 hw.audioInput = no hw.audioOutput = no hw.accelerometer = no hw.gyroscope = no hw.dPad = no hw.mainKeys = yes hw.keyboard = no hw.sensors.proximity = no hw.sensors.magnetic_field = no hw.sensors.orientation = no hw.sensors.temperature = no hw.sensors.light = no hw.sensors.pressure = no hw.sensors.humidity = no hw.sensors.magnetic_field_uncalibrated = no hw.sensors.gyroscope_uncalibrated = no image.sysdir.1 = system-images/android-29/google_apis/x86_64/ tag.display = Google APIs tag.id = google_apis skin.dynamic = yes skin.name = 320x480 disk.dataPartition.size = 8G Pay your attention that we disabled: Accelerometer Audio input/output Play Store Sensors:Accelerometer, Humidity, Pressure, Light Gyroscope We don't really need them for our tests run. It also may improve our tests performance, because there are no background operations related to that tasks. After that, you can run your emulator by avd manager , which is a part of android sdk manager. After your device creation, you need change default generated ini file to custom one. You may see an example below: function define_android_sdk_environment_if_needed () { android_env = $( printenv ANDROID_HOME ) if [ -z \" $android_env \" ] ; then if [[ -d \" $HOME /Library/Android/sdk\" ]] ; then export ANDROID_HOME = \" $HOME /Library/Android/sdk\" else printf \"Can't locate your android sdk. Please set ANDROID_HOME manually\" exit fi fi } function define_path_environment_if_needed () { if ! command -v adb & >/dev/null ; then export PATH = ~/Library/Android/sdk/tools: $PATH export PATH = ~/Library/Android/sdk/platform-tools: $PATH fi } function create_and_patch_emulator () { EMULATOR_API_VERSION = 29 EMULATOR_NAME = \"ui_tests_emulator_api_ ${ EMULATOR_API_VERSION } \" # Install sdk $ANDROID_HOME /cmdline-tools/latest/bin/sdkmanager \"system-images;android- ${ EMULATOR_API_VERSION } ;google_apis;x86_64\" # Create new emulator echo \"no\" | $ANDROID_HOME /cmdline-tools/latest/bin/avdmanager create avd --force \\ --name \" ${ EMULATOR_NAME } \" \\ --package \"system-images;android- ${ EMULATOR_API_VERSION } ;google_apis;x86_64\" \\ --abi google_apis/x86_64 # Change emulator's config after emulator's creation cp -p YOUR_INI_FILE.ini $HOME /.android/avd/ ${ EMULATOR_NAME } .avd/config.ini # Run new emulator pushd ${ ANDROID_HOME } /emulator/ nohup ${ ANDROID_HOME } /emulator/emulator @ ${ EMULATOR_NAME } >/dev/null 2 > & 1 & } define_android_sdk_environment_if_needed define_path_environment_if_needed create_and_patch_emulator Pay your attention that you also need to wait until your emulator is fully booted. How to run an emulator in a Docker? Running an emulator in a docker a way easier than manually, because it encapsulates all this logic. If you don't have an experience with docker, you can check this guide to check the basics. There are some popular already built docker images for you: Official Google emulator Agoda emulator Avito emulator Talking about Avito emulator , it also patches your emulator with adb commands to prevent tests flakiness and to speed them up Run Avito emulator #run emulator 1 in a headless mode docker run -d -p 5555 :5555 -p 5554 :5554 -p 8554 :8554 --privileged avitotech/android-emulator-29:915c1f20be adb connect localhost:5555 #run emulator 2 in a headless mode docker run -d -p 5557 :5555 -p 5556 :5554 -p 8555 :8554 --privileged avitotech/android-emulator-29:915c1f20be adb connect localhost:5557 #run emulator 3 in a headless mode docker run -d -p 5559 :5555 -p 5558 :5554 -p 8556 :8554 --privileged avitotech/android-emulator-29:915c1f20be adb connect localhost:5559 #...etc Stop all emulators docker kill $( docker ps -q ) docker rm $( docker ps -a -q ) Conclusion Use docker emulators You also will have an opportunity to run them with Kubernetes , to make it scalable in the future Start fresh emulators each test batch and kill them after all of your tests finished Emulators tend to leak and may not work properly after some time Use the same emulator as on CI locally All devices are different. It can save you a lot of time with debugging and understanding why your test works locally and fails on CI. It won't be possible to run Docker emulator on macOS or Windows, because of haxm#51 . Use AVD to launch them on such machines (script above may help you) Warning To run an emulator on CI with a docker, make sure that nested virtualisation supported and KVM installed. You can check more details here","title":"Emulator setup"},{"location":"practices/emulator_setup/#emulator-setup","text":"Basically we have next choices: Manage devices automatically by avd Manage docker containers with emulators by docker Using docker image is the easiest way, however it's important to understand how docker creates device for you.","title":"Emulator setup"},{"location":"practices/emulator_setup/#creating-an-emulator","text":"Before starting to read this topic, make sure you've read an an official documentation Firstly, you need to create an ini configuration: PlayStore.enabled = false abi.type = x86_64 avd.ini.encoding = UTF-8 hw.cpu.arch = x86_64 hw.cpu.ncore = 2 hw.ramSize = 2048 hw.lcd.density = 120 hw.lcd.width = 320 hw.lcd.height = 480 hw.audioInput = no hw.audioOutput = no hw.accelerometer = no hw.gyroscope = no hw.dPad = no hw.mainKeys = yes hw.keyboard = no hw.sensors.proximity = no hw.sensors.magnetic_field = no hw.sensors.orientation = no hw.sensors.temperature = no hw.sensors.light = no hw.sensors.pressure = no hw.sensors.humidity = no hw.sensors.magnetic_field_uncalibrated = no hw.sensors.gyroscope_uncalibrated = no image.sysdir.1 = system-images/android-29/google_apis/x86_64/ tag.display = Google APIs tag.id = google_apis skin.dynamic = yes skin.name = 320x480 disk.dataPartition.size = 8G Pay your attention that we disabled: Accelerometer Audio input/output Play Store Sensors:Accelerometer, Humidity, Pressure, Light Gyroscope We don't really need them for our tests run. It also may improve our tests performance, because there are no background operations related to that tasks. After that, you can run your emulator by avd manager , which is a part of android sdk manager. After your device creation, you need change default generated ini file to custom one. You may see an example below: function define_android_sdk_environment_if_needed () { android_env = $( printenv ANDROID_HOME ) if [ -z \" $android_env \" ] ; then if [[ -d \" $HOME /Library/Android/sdk\" ]] ; then export ANDROID_HOME = \" $HOME /Library/Android/sdk\" else printf \"Can't locate your android sdk. Please set ANDROID_HOME manually\" exit fi fi } function define_path_environment_if_needed () { if ! command -v adb & >/dev/null ; then export PATH = ~/Library/Android/sdk/tools: $PATH export PATH = ~/Library/Android/sdk/platform-tools: $PATH fi } function create_and_patch_emulator () { EMULATOR_API_VERSION = 29 EMULATOR_NAME = \"ui_tests_emulator_api_ ${ EMULATOR_API_VERSION } \" # Install sdk $ANDROID_HOME /cmdline-tools/latest/bin/sdkmanager \"system-images;android- ${ EMULATOR_API_VERSION } ;google_apis;x86_64\" # Create new emulator echo \"no\" | $ANDROID_HOME /cmdline-tools/latest/bin/avdmanager create avd --force \\ --name \" ${ EMULATOR_NAME } \" \\ --package \"system-images;android- ${ EMULATOR_API_VERSION } ;google_apis;x86_64\" \\ --abi google_apis/x86_64 # Change emulator's config after emulator's creation cp -p YOUR_INI_FILE.ini $HOME /.android/avd/ ${ EMULATOR_NAME } .avd/config.ini # Run new emulator pushd ${ ANDROID_HOME } /emulator/ nohup ${ ANDROID_HOME } /emulator/emulator @ ${ EMULATOR_NAME } >/dev/null 2 > & 1 & } define_android_sdk_environment_if_needed define_path_environment_if_needed create_and_patch_emulator Pay your attention that you also need to wait until your emulator is fully booted.","title":"Creating an emulator"},{"location":"practices/emulator_setup/#how-to-run-an-emulator-in-a-docker","text":"Running an emulator in a docker a way easier than manually, because it encapsulates all this logic. If you don't have an experience with docker, you can check this guide to check the basics. There are some popular already built docker images for you: Official Google emulator Agoda emulator Avito emulator Talking about Avito emulator , it also patches your emulator with adb commands to prevent tests flakiness and to speed them up","title":"How to run an emulator in a Docker?"},{"location":"practices/emulator_setup/#run-avito-emulator","text":"#run emulator 1 in a headless mode docker run -d -p 5555 :5555 -p 5554 :5554 -p 8554 :8554 --privileged avitotech/android-emulator-29:915c1f20be adb connect localhost:5555 #run emulator 2 in a headless mode docker run -d -p 5557 :5555 -p 5556 :5554 -p 8555 :8554 --privileged avitotech/android-emulator-29:915c1f20be adb connect localhost:5557 #run emulator 3 in a headless mode docker run -d -p 5559 :5555 -p 5558 :5554 -p 8556 :8554 --privileged avitotech/android-emulator-29:915c1f20be adb connect localhost:5559 #...etc","title":"Run Avito emulator"},{"location":"practices/emulator_setup/#stop-all-emulators","text":"docker kill $( docker ps -q ) docker rm $( docker ps -a -q )","title":"Stop all emulators"},{"location":"practices/emulator_setup/#conclusion","text":"Use docker emulators You also will have an opportunity to run them with Kubernetes , to make it scalable in the future Start fresh emulators each test batch and kill them after all of your tests finished Emulators tend to leak and may not work properly after some time Use the same emulator as on CI locally All devices are different. It can save you a lot of time with debugging and understanding why your test works locally and fails on CI. It won't be possible to run Docker emulator on macOS or Windows, because of haxm#51 . Use AVD to launch them on such machines (script above may help you) Warning To run an emulator on CI with a docker, make sure that nested virtualisation supported and KVM installed. You can check more details here","title":"Conclusion"},{"location":"practices/emulator_vs_real_device/","text":"Emulator vs Real device This question is a trade off and there is no right and wrong answers. We'll review pros/cons and basic emulator setup on CI Real device Here is pros/cons \u2795 Real environment \u2795 Doesn't consume CI resources \u2796 Breaks often \u2796 Requires special conditions A real device will help you to catch more bugs from the first perspective, however talking about scalability, if you have a lot of devices, you need to locate them in a special room with no direct sunlight and with a climate-control. However, it doesn't save from disk and battery deterioration, because they are always charging and performs I/O operations. It may be a reason of your tests failing not because of them caught a real bug, but because of an issue with a device. Emulator Here is pros/cons \u2795 Easy configurable \u2795 Can work faster than a real device Keep in mind that it's achievable only if you applied a special configuration and have powerful build agents \u2795 \u0422ot demanding in maintenance \u2796 Not a real environment \u2796 Consumes CI resources The most benefit that we may have is a fresh emulator instance each test bunch. Also, it's possible to create a special configuration and disable features you don't need to have in tests which affects device stability. However, you need to have powerful machine (and definitely not one, if you want to run your tests on pull requests)","title":"Emulator vs real device"},{"location":"practices/emulator_vs_real_device/#emulator-vs-real-device","text":"This question is a trade off and there is no right and wrong answers. We'll review pros/cons and basic emulator setup on CI","title":"Emulator vs Real device"},{"location":"practices/emulator_vs_real_device/#real-device","text":"Here is pros/cons \u2795 Real environment \u2795 Doesn't consume CI resources \u2796 Breaks often \u2796 Requires special conditions A real device will help you to catch more bugs from the first perspective, however talking about scalability, if you have a lot of devices, you need to locate them in a special room with no direct sunlight and with a climate-control. However, it doesn't save from disk and battery deterioration, because they are always charging and performs I/O operations. It may be a reason of your tests failing not because of them caught a real bug, but because of an issue with a device.","title":"Real device"},{"location":"practices/emulator_vs_real_device/#emulator","text":"Here is pros/cons \u2795 Easy configurable \u2795 Can work faster than a real device Keep in mind that it's achievable only if you applied a special configuration and have powerful build agents \u2795 \u0422ot demanding in maintenance \u2796 Not a real environment \u2796 Consumes CI resources The most benefit that we may have is a fresh emulator instance each test bunch. Also, it's possible to create a special configuration and disable features you don't need to have in tests which affects device stability. However, you need to have powerful machine (and definitely not one, if you want to run your tests on pull requests)","title":"Emulator"},{"location":"practices/flakiness/","text":"Flakiness Flakiness it's an unstable behavior of particular test. If you execute this test N times, it won't pass N/N . Or, it can pass only locally, but always or often failed on the CI. It's the most frustrating problem in instrumented testing, which requires a lot of time from engineers to fight. Reason Production code Example: async operations, race-conditions Test code Example: testing toasts/snack-bars A real device or Emulator Example: Disk/Battery/Processor/Memory issues or notification has shown on the device Infrastructure Example: Processor/Disk/Memory issues It's not possible to fight flakiness on 100% if your codebase changes every day (including the new sources of flakiness) However, it's possible to reduce it and achieve good percentage of flakiness free. In general, to reduce flakiness you need to choose tools like framework for writing, test runner and emulator properly Flakiness protection 1. Wait for the content appearing When we have an http request or other asynchronous operation, it's not possible to predict how soon our expected content will be shown on the screen. By default, Espresso framework will fail assertion if there is no expected content in a particular time. Google provided Idling Resources to catch asynchronous operations. However, this goes against the common testing best practice of not putting testing code inside your application code and also requires an additional effort from engineers. Recommended by community way it's to use smart-waiting (aka flaky safely algorithm) like this : fun < T > invokeFlakySafely ( params : FlakySafetyParams , failureMessage : String? = null , action : () -> T ): T { var cachedError : Throwable val startTime = System . currentTimeMillis () do { try { return action . invoke () } catch ( error : Throwable ) { if ( error . isAllowed ( params . allowedExceptions )) { cachedError = error lock . withLock { Timer (). schedule ( params . intervalMs ) { lock . withLock { condition . signalAll () } } condition . await () } } else { throw error } } } while ( System . currentTimeMillis () - startTime <= params . timeoutMs ) throw cachedError . withMessage ( failureMessage ) } This is an internals of the Kaspresso library Official documentation says that it's not a good way to handle this, because of an additional consuming of CPU resources. However, it's a pragmatic trade-off which speed ui testing writing up and relieves engineers from thinking about this problem at all. Some frameworks have already implemented solution, which intercepts all assertions: Avito UI test framework Kaspresso Consider using them to avoid this problem at all. 2. Use isolated environment for each test Package clear before each test will all your data in application and process itself. This will get rid of the likelihood affection old data to your current test. Marathon and Avito-Test runner provide the easiest way to clear the state. You can see the details here: State Clearing 3. Test vanishing content in other way (Toasts, Snackbars, etc) Testing the content which is going to be hidden after N time (usually ms) it's also challenging. Toast might be shown properly, but your test framework is checking other content on the screen at the particular moment. When this check is done, toast might have already been disappeared, your test will be failed. To solve this, you may not to test it at all. Or, you can have some proxy object which saves a fact that Toast/SnackBar has been shown. This solution has already been implemented by Avito company, you may check the details here If you have own designed component, which is also disappears after some time, you can disable this disparity for tests and close it manually. 4. Use special configuration for your device In the most of the cases you don't to have Accelerometer, Audio input/output, Play Store, Sensors and Gyroscope in your tests. You can see how to disable them here: Emulator setup Also, it's recommended way to disable animations on the device, screen-off timeout and long press timeout. The script below will patch all your devices connected to adb devices = $( adb devices -l | sed '1d' | sed '$d' | awk '{print $1}' ) for d in $devices ; do adb -s \" $d \" shell \"settings put global window_animation_scale 0.0\" adb -s \" $d \" shell \"settings put global transition_animation_scale 0.0\" adb -s \" $d \" shell \"settings put global animator_duration_scale 0.0\" adb -s \" $d \" shell \"settings put secure spell_checker_enabled 0\" adb -s \" $d \" shell \"settings put secure show_ime_with_hard_keyboard 1\" adb -s \" $d \" shell \"settings put system screen_off_timeout 2147483647\" adb -s \" $d \" shell \"settings put secure long_press_timeout 1500\" adb -s \" $d \" shell \"settings put global hidden_api_policy_pre_p_apps 1\" adb -s \" $d \" shell \"settings put global hidden_api_policy_p_apps 1\" adb -s \" $d \" shell \"settings put global hidden_api_policy 1\" done 5. Use fresh emulator instance each test batch Your tests may affect your emulator work, like save some information in the external storage, which can be a reason of flakiness. It's not pragmatic to run a new emulator for each test in terms of speed, however you can do it each batch. Just kill emulators when all of your tests finished. You can see how to disable them here: Emulator setup 6. Mock your network layer In 2021, it's still not possible to have stable network connection. To achieve stability, it's better to mock it. Yes, after that our test is not fully end-to-end, but it's a pragmatic trade-off. You can read more about it here: Network 7. Close system tray notifications before each test This problem may appear if some of your tests for some reasons didn't close the system notification tray. All of the next tests will be failed because of this. To prevent such case, you can write a test rule which will close such notification before each test : class CloseNotificationsRule : ExternalResource () { override fun before () { UiDevice . getInstance ( InstrumentationRegistry . getInstrumentation ()) . pressHome () } } 8. Use retries Retry it's a last of flakiness protection layer. It's better to delegate it to test runner instead of custom test rule, as our test process might be crashed during the test execution. If test passed as minimum once, we consider it as passed. It's recommended by the community way to always have as minimum as one retry. As we showed before, it's not possible to fight flakiness in 100%, if your codebase changes really often. You also may have 100% flakiness free if you use only one device, but you might have some problems if you run your tests across multiple devices because them consume more resources. Usually tests are flaky in a not really convenient way. If you have UI tests as a part of CD, your release will be automatically blocked because of it. Do not avoid retries. Try to reduce them and always check, why test has been retried. You can read more about retries and flakiness strategies here: Test runners 9. Avoid bottlenecks Imagine you have a test which navigates to your feature throughout MainScreen . MainScreen it's a bottleneck, because a lot of teams can be contributing in there. Try to open your particular feature directly in your tests. You can do it via ActivityScenario , or by using the same components as using in deeplink processing, or by using custom created navigation component. However, leave as minimum as 1 test, which checks that your feature can be opened from MainScreen 10. Sort your tests It also can be a reason of flakiness, if you run your tests across multiple devices. Especially, when you run test with different execution time in parallel. While test1 is running, test2, test3, test4 can be finished. Test runners like Marathon/Avito will pull the device data after that, which can create artificially created delay, which can be a reason of test1 failing. Sorting test by execution time based on a previous run will reduce the count of issues like this. 11. Use the same emulator configuration locally and on the CI Test can work fine in one device, however it can be failed on another device. Try to use an emulator with the same configuration as on CI locally. You also can add some checks, which prohibit to run instrumented tests locally not on the same emulator as on CI. devices = $( adb devices -l | sed '1d' | sed '$d' | awk '{print $1}' ) for d in $devices ; do device_version = $( adb -s \" $d \" shell getprop ro.build.version.sdk ) emulator_name = $( adb -s \" $d \" shell getprop ro.kernel.qemu.avd_name ) if [ \" $emulator_name \" ! = $ANDROID_ALLOWED_EMULATOR_29 ] ; then throw_error \"One of connected to adb devices not supported to run UI tests, please disconnect them and run emulator, using: ./runEmulator.sh\" fi if [ \" $device_version \" ! = 29 ] ; then throw_error \"Please, use emulator with sdk 29 as the same version uses on verification on CI. To create emulator, use: ./runEmulator --ui-test\" fi done 12. Use the same test runner locally and on the CI Your test launch on CI and locally shouldn't be different. If you use 3rd party test runner on CI, use it to run your tests locally as well 13. Collect and observe flakiness information Always monitor flakiness percentage to reduce them and try to automate it. Marathon provides an information about retries it's done during the test run in a report meta-files in json format. Using them, you can create a Slack notification which posts some data with flakiness free: Flakiness report: Flakiness free: 95 % ( tests passed from 1st attempt ) Flakiness overhead: 25m:1s ( how much time we spent on retries ) Average succeed test execution time: 29s ActionsInChatTest#chat_is_read_only_no_input passed from 3 attempt ReplaceCard#checkSelectReplaceCardReasonScreenOpened passed from 2 attempt NewChatTest#new_chat_from_help_screen_created_with_written_suggestion passed from 2 attempt ExistingChatTest#chat_ongoing_from_all_requests_screen_opened passed from 2 attempt 14. Validate all tests for flakiness At night, when engineers sleep, you can trigger a CI job which runs all of your tests N times (like 10-30-100). Marathon provides the most convenient way to do that. You can read more about it here: Test runners","title":"Flakiness"},{"location":"practices/flakiness/#flakiness","text":"Flakiness it's an unstable behavior of particular test. If you execute this test N times, it won't pass N/N . Or, it can pass only locally, but always or often failed on the CI. It's the most frustrating problem in instrumented testing, which requires a lot of time from engineers to fight.","title":"Flakiness"},{"location":"practices/flakiness/#reason","text":"Production code Example: async operations, race-conditions Test code Example: testing toasts/snack-bars A real device or Emulator Example: Disk/Battery/Processor/Memory issues or notification has shown on the device Infrastructure Example: Processor/Disk/Memory issues It's not possible to fight flakiness on 100% if your codebase changes every day (including the new sources of flakiness) However, it's possible to reduce it and achieve good percentage of flakiness free. In general, to reduce flakiness you need to choose tools like framework for writing, test runner and emulator properly","title":"Reason"},{"location":"practices/flakiness/#flakiness-protection","text":"","title":"Flakiness protection"},{"location":"practices/flakiness/#1-wait-for-the-content-appearing","text":"When we have an http request or other asynchronous operation, it's not possible to predict how soon our expected content will be shown on the screen. By default, Espresso framework will fail assertion if there is no expected content in a particular time. Google provided Idling Resources to catch asynchronous operations. However, this goes against the common testing best practice of not putting testing code inside your application code and also requires an additional effort from engineers. Recommended by community way it's to use smart-waiting (aka flaky safely algorithm) like this : fun < T > invokeFlakySafely ( params : FlakySafetyParams , failureMessage : String? = null , action : () -> T ): T { var cachedError : Throwable val startTime = System . currentTimeMillis () do { try { return action . invoke () } catch ( error : Throwable ) { if ( error . isAllowed ( params . allowedExceptions )) { cachedError = error lock . withLock { Timer (). schedule ( params . intervalMs ) { lock . withLock { condition . signalAll () } } condition . await () } } else { throw error } } } while ( System . currentTimeMillis () - startTime <= params . timeoutMs ) throw cachedError . withMessage ( failureMessage ) } This is an internals of the Kaspresso library Official documentation says that it's not a good way to handle this, because of an additional consuming of CPU resources. However, it's a pragmatic trade-off which speed ui testing writing up and relieves engineers from thinking about this problem at all. Some frameworks have already implemented solution, which intercepts all assertions: Avito UI test framework Kaspresso Consider using them to avoid this problem at all.","title":"1. Wait for the content appearing "},{"location":"practices/flakiness/#2-use-isolated-environment-for-each-test","text":"Package clear before each test will all your data in application and process itself. This will get rid of the likelihood affection old data to your current test. Marathon and Avito-Test runner provide the easiest way to clear the state. You can see the details here: State Clearing","title":"2. Use isolated environment for each test "},{"location":"practices/flakiness/#3-test-vanishing-content-in-other-way-toasts-snackbars-etc","text":"Testing the content which is going to be hidden after N time (usually ms) it's also challenging. Toast might be shown properly, but your test framework is checking other content on the screen at the particular moment. When this check is done, toast might have already been disappeared, your test will be failed. To solve this, you may not to test it at all. Or, you can have some proxy object which saves a fact that Toast/SnackBar has been shown. This solution has already been implemented by Avito company, you may check the details here If you have own designed component, which is also disappears after some time, you can disable this disparity for tests and close it manually.","title":"3. Test vanishing content in other way (Toasts, Snackbars, etc) "},{"location":"practices/flakiness/#4-use-special-configuration-for-your-device","text":"In the most of the cases you don't to have Accelerometer, Audio input/output, Play Store, Sensors and Gyroscope in your tests. You can see how to disable them here: Emulator setup Also, it's recommended way to disable animations on the device, screen-off timeout and long press timeout. The script below will patch all your devices connected to adb devices = $( adb devices -l | sed '1d' | sed '$d' | awk '{print $1}' ) for d in $devices ; do adb -s \" $d \" shell \"settings put global window_animation_scale 0.0\" adb -s \" $d \" shell \"settings put global transition_animation_scale 0.0\" adb -s \" $d \" shell \"settings put global animator_duration_scale 0.0\" adb -s \" $d \" shell \"settings put secure spell_checker_enabled 0\" adb -s \" $d \" shell \"settings put secure show_ime_with_hard_keyboard 1\" adb -s \" $d \" shell \"settings put system screen_off_timeout 2147483647\" adb -s \" $d \" shell \"settings put secure long_press_timeout 1500\" adb -s \" $d \" shell \"settings put global hidden_api_policy_pre_p_apps 1\" adb -s \" $d \" shell \"settings put global hidden_api_policy_p_apps 1\" adb -s \" $d \" shell \"settings put global hidden_api_policy 1\" done","title":"4. Use special configuration for your device "},{"location":"practices/flakiness/#5-use-fresh-emulator-instance-each-test-batch","text":"Your tests may affect your emulator work, like save some information in the external storage, which can be a reason of flakiness. It's not pragmatic to run a new emulator for each test in terms of speed, however you can do it each batch. Just kill emulators when all of your tests finished. You can see how to disable them here: Emulator setup","title":"5. Use fresh emulator instance each test batch "},{"location":"practices/flakiness/#6-mock-your-network-layer","text":"In 2021, it's still not possible to have stable network connection. To achieve stability, it's better to mock it. Yes, after that our test is not fully end-to-end, but it's a pragmatic trade-off. You can read more about it here: Network","title":"6. Mock your network layer "},{"location":"practices/flakiness/#7-close-system-tray-notifications-before-each-test","text":"This problem may appear if some of your tests for some reasons didn't close the system notification tray. All of the next tests will be failed because of this. To prevent such case, you can write a test rule which will close such notification before each test : class CloseNotificationsRule : ExternalResource () { override fun before () { UiDevice . getInstance ( InstrumentationRegistry . getInstrumentation ()) . pressHome () } }","title":"7. Close system tray notifications before each test "},{"location":"practices/flakiness/#8-use-retries","text":"Retry it's a last of flakiness protection layer. It's better to delegate it to test runner instead of custom test rule, as our test process might be crashed during the test execution. If test passed as minimum once, we consider it as passed. It's recommended by the community way to always have as minimum as one retry. As we showed before, it's not possible to fight flakiness in 100%, if your codebase changes really often. You also may have 100% flakiness free if you use only one device, but you might have some problems if you run your tests across multiple devices because them consume more resources. Usually tests are flaky in a not really convenient way. If you have UI tests as a part of CD, your release will be automatically blocked because of it. Do not avoid retries. Try to reduce them and always check, why test has been retried. You can read more about retries and flakiness strategies here: Test runners","title":"8. Use retries "},{"location":"practices/flakiness/#9-avoid-bottlenecks","text":"Imagine you have a test which navigates to your feature throughout MainScreen . MainScreen it's a bottleneck, because a lot of teams can be contributing in there. Try to open your particular feature directly in your tests. You can do it via ActivityScenario , or by using the same components as using in deeplink processing, or by using custom created navigation component. However, leave as minimum as 1 test, which checks that your feature can be opened from MainScreen","title":"9. Avoid bottlenecks"},{"location":"practices/flakiness/#10-sort-your-tests","text":"It also can be a reason of flakiness, if you run your tests across multiple devices. Especially, when you run test with different execution time in parallel. While test1 is running, test2, test3, test4 can be finished. Test runners like Marathon/Avito will pull the device data after that, which can create artificially created delay, which can be a reason of test1 failing. Sorting test by execution time based on a previous run will reduce the count of issues like this.","title":"10. Sort your tests "},{"location":"practices/flakiness/#11-use-the-same-emulator-configuration-locally-and-on-the-ci","text":"Test can work fine in one device, however it can be failed on another device. Try to use an emulator with the same configuration as on CI locally. You also can add some checks, which prohibit to run instrumented tests locally not on the same emulator as on CI. devices = $( adb devices -l | sed '1d' | sed '$d' | awk '{print $1}' ) for d in $devices ; do device_version = $( adb -s \" $d \" shell getprop ro.build.version.sdk ) emulator_name = $( adb -s \" $d \" shell getprop ro.kernel.qemu.avd_name ) if [ \" $emulator_name \" ! = $ANDROID_ALLOWED_EMULATOR_29 ] ; then throw_error \"One of connected to adb devices not supported to run UI tests, please disconnect them and run emulator, using: ./runEmulator.sh\" fi if [ \" $device_version \" ! = 29 ] ; then throw_error \"Please, use emulator with sdk 29 as the same version uses on verification on CI. To create emulator, use: ./runEmulator --ui-test\" fi done","title":"11. Use the same emulator configuration locally and on the CI "},{"location":"practices/flakiness/#12-use-the-same-test-runner-locally-and-on-the-ci","text":"Your test launch on CI and locally shouldn't be different. If you use 3rd party test runner on CI, use it to run your tests locally as well","title":"12. Use the same test runner locally and on the CI "},{"location":"practices/flakiness/#13-collect-and-observe-flakiness-information","text":"Always monitor flakiness percentage to reduce them and try to automate it. Marathon provides an information about retries it's done during the test run in a report meta-files in json format. Using them, you can create a Slack notification which posts some data with flakiness free: Flakiness report: Flakiness free: 95 % ( tests passed from 1st attempt ) Flakiness overhead: 25m:1s ( how much time we spent on retries ) Average succeed test execution time: 29s ActionsInChatTest#chat_is_read_only_no_input passed from 3 attempt ReplaceCard#checkSelectReplaceCardReasonScreenOpened passed from 2 attempt NewChatTest#new_chat_from_help_screen_created_with_written_suggestion passed from 2 attempt ExistingChatTest#chat_ongoing_from_all_requests_screen_opened passed from 2 attempt","title":"13. Collect and observe flakiness information "},{"location":"practices/flakiness/#14-validate-all-tests-for-flakiness","text":"At night, when engineers sleep, you can trigger a CI job which runs all of your tests N times (like 10-30-100). Marathon provides the most convenient way to do that. You can read more about it here: Test runners","title":"14. Validate all tests for flakiness "},{"location":"practices/network/","text":"Network UI testing can play a role of regular auto-testing in your project. Your app communicates with external services to retrieve data e.g. user profile after login. This communication happens asynchronously, what hinders Ui Testing as explained below. In that case, you have to choose, how to deal with network connections. Problems Ideally, we want to achieve the next picture: As we can see, all levels of the application are covered, the test is fully integrated, therefore, more reliable, with a product quality point of view. If the backend changed the contract, we will know about it right away. Unfortunately, this has a downside: everything about the network is unstable. The internet suddenly works a lot of worse or stop working altogether, or your backend may have redeployment step. If you make GET and UPDATE operations on the same data, the next time the test runs, the GET operation will run with the updated value, what might make the test fail, since when it was written it was expecting to GET the non-updated data. All of this can cause your test to fail and show a non-representative false result. On the other side, if you completely abandon the network, you cannot name it E2E test. Like everything in UI tests, networking is a trade-off. Unfortunately, here it will not be possible to find a 10your0% correct solution that will be simultaneously stable and test every layer in your app at the same time. Let's look, which options do we have Real network Production server You will get 100% E2E . On the other hand, not all applications will be able to do this (for example, in the banking community) Development server We still have E2E , but the environment can be significantly different from production. Also, regular redeployment can be a problem Special stage server Can be different with your production/development backend. Also pay your, that it requires an additional effort from engineers to support it. It also is not E2e, as it tests not a real backend side. Warning It doesn't matter which of the options above you choose: Internet connection always can be an issue, because you cannot guarantee stability at any time. If you use UI testing as a part of pull request or release pipeline, it's better to avoid real network usage there, but just run them regularly several times a day Mock network (manually) MockWebServer Part of OkHttp library. Allows intercepting and responding of each http request. You can find more details about it here Custom Interceptor In the simplest implementation, this is a common interceptor which is intercepts each http call, which is added only in tests and actually replaces triggered requests for predefined responses. However, unlike OkHttpWebServer , it works for more high level, therefore, may miss some bugs. Also, in the most of the cases, we don't need to test third party libraries code DI Each DI framework ( Dagger2 / Toothpick / Koin / etc) allows you to replace dependencies in tests. We can replace the Retroift Service or the whole Repository entirely. However, we actually crop testing the entire network layer and offline mode if it exists Problems You need to mock manually. Imagine, that in your flow has hundreds of http requests. You may spend a lot of time mocking every single http response, and it's really challenging to keep these tests up-to-date. Mock network (automatically) The key idea is that two modes of network operation appear in the tests: Record & Replay . Record In this mode, the test uses a real backend. However, it also records all requests and exports them to set or to one file that can be associated with a specific test. This mode is used locally only when writing a test Replay Instead of a real network, an already recorded file is used. This mode is used both locally and on CI. It allows to fully exclude all the possible problems with a network. Also, we've got an opportunity to easily mock authorisation and skip some on-boarding screens in this mode, because we have a full picture in terms of http requests. Instruments: Wiremock \u2795 Works on the system level ( MITM internally) \u2795 Record/Replay from the box \u2795 Keeping request time in playback mode \u2795 Request indexing (the same request can be recorded with different answers, if it executes multiple times) \u2795 Smart request substitution (Will substitute similar request if particular one hasn't been found) \u2795 Manually mocking support \u2796 Not Android friendly \u2796 Doesn't support multiple hosts from the box \u2796 Doesn't convenient output (set of json files) \u2796 Requires a custom implementation of storing files You may find an example how to implement it here OkReplay \u2795 Android Friendly, easy adoption \u2795 OkHttp interceptor internally \u2795 Multiple hosts from the box (because of interceptor) \u2795 Readable and editable output ( .yaml file with all recorded requests and responses in a one place) \u2795 Opportunity to pull stubs from the device and store them to the device \u2796 Doesn't maintain \u2796 Doesn't have request indexing \u2796 Doesn't keep http request time \u2796 Doesn't have manually mocking \u2796 Doesn't have smart indexing Solution from Revolut (Work in progress) Revolut prepares an open source library, which allows getting rid of all problems mentioned above and make adoption a way easier Problems Account preparation You need to prepare a user which matches the special condition you need to test. It can be still mocked manually or via specially created service, which does this job for you Core changes As soon as your root code base changed in terms of http requests, you probably would need to re-record some tests. That's why it's better to skip authorisation mode in the playback mode, to reduce such cases Conclusion No perfect solution exists, it all depends on a number of factors, for example, the size of the team and how often the code base changes. It also depends on what goals you set in UI testing. Should I avoid real network testing in general? Depends on UI testing goals, project type and testing priorities. If it's important for you to have UI testing as a part of Pull Request or Release pipelines, then it's very important to put the stability of these tests first a place. Mocking the network, you sacrifice some uncaught bugs, but you get stability and speed in return. These tests will not catch a bug when something has changed on the backend, but you can test how your product is ready for a change contract and nullable/non-nullable fields. On the other hand, if you do not need a full automation and having UI testing as a part of it, and at the same time an option to use production backend, tests with a real network are all still have the right to life in your product. I have a small project. Which tool can I use for mock? If you have not a lot of http requests, like 5-20 on application start, MockWebServer will be pragmatic option I have a lot of http requests, my codebase changes really often Record/Replay practice will be useful in that case. Check OkReplay or Wiremock solution. Also, you can combine approaches: Record & Replay, and mock only specific requests. You also may check, what approaches are used by other companies","title":"Network"},{"location":"practices/network/#network","text":"UI testing can play a role of regular auto-testing in your project. Your app communicates with external services to retrieve data e.g. user profile after login. This communication happens asynchronously, what hinders Ui Testing as explained below. In that case, you have to choose, how to deal with network connections.","title":"Network"},{"location":"practices/network/#problems","text":"Ideally, we want to achieve the next picture: As we can see, all levels of the application are covered, the test is fully integrated, therefore, more reliable, with a product quality point of view. If the backend changed the contract, we will know about it right away. Unfortunately, this has a downside: everything about the network is unstable. The internet suddenly works a lot of worse or stop working altogether, or your backend may have redeployment step. If you make GET and UPDATE operations on the same data, the next time the test runs, the GET operation will run with the updated value, what might make the test fail, since when it was written it was expecting to GET the non-updated data. All of this can cause your test to fail and show a non-representative false result. On the other side, if you completely abandon the network, you cannot name it E2E test. Like everything in UI tests, networking is a trade-off. Unfortunately, here it will not be possible to find a 10your0% correct solution that will be simultaneously stable and test every layer in your app at the same time. Let's look, which options do we have","title":"Problems"},{"location":"practices/network/#real-network","text":"Production server You will get 100% E2E . On the other hand, not all applications will be able to do this (for example, in the banking community) Development server We still have E2E , but the environment can be significantly different from production. Also, regular redeployment can be a problem Special stage server Can be different with your production/development backend. Also pay your, that it requires an additional effort from engineers to support it. It also is not E2e, as it tests not a real backend side. Warning It doesn't matter which of the options above you choose: Internet connection always can be an issue, because you cannot guarantee stability at any time. If you use UI testing as a part of pull request or release pipeline, it's better to avoid real network usage there, but just run them regularly several times a day","title":"Real network"},{"location":"practices/network/#mock-network-manually","text":"MockWebServer Part of OkHttp library. Allows intercepting and responding of each http request. You can find more details about it here Custom Interceptor In the simplest implementation, this is a common interceptor which is intercepts each http call, which is added only in tests and actually replaces triggered requests for predefined responses. However, unlike OkHttpWebServer , it works for more high level, therefore, may miss some bugs. Also, in the most of the cases, we don't need to test third party libraries code DI Each DI framework ( Dagger2 / Toothpick / Koin / etc) allows you to replace dependencies in tests. We can replace the Retroift Service or the whole Repository entirely. However, we actually crop testing the entire network layer and offline mode if it exists Problems You need to mock manually. Imagine, that in your flow has hundreds of http requests. You may spend a lot of time mocking every single http response, and it's really challenging to keep these tests up-to-date.","title":"Mock network (manually)"},{"location":"practices/network/#mock-network-automatically","text":"The key idea is that two modes of network operation appear in the tests: Record & Replay . Record In this mode, the test uses a real backend. However, it also records all requests and exports them to set or to one file that can be associated with a specific test. This mode is used locally only when writing a test Replay Instead of a real network, an already recorded file is used. This mode is used both locally and on CI. It allows to fully exclude all the possible problems with a network. Also, we've got an opportunity to easily mock authorisation and skip some on-boarding screens in this mode, because we have a full picture in terms of http requests. Instruments: Wiremock \u2795 Works on the system level ( MITM internally) \u2795 Record/Replay from the box \u2795 Keeping request time in playback mode \u2795 Request indexing (the same request can be recorded with different answers, if it executes multiple times) \u2795 Smart request substitution (Will substitute similar request if particular one hasn't been found) \u2795 Manually mocking support \u2796 Not Android friendly \u2796 Doesn't support multiple hosts from the box \u2796 Doesn't convenient output (set of json files) \u2796 Requires a custom implementation of storing files You may find an example how to implement it here OkReplay \u2795 Android Friendly, easy adoption \u2795 OkHttp interceptor internally \u2795 Multiple hosts from the box (because of interceptor) \u2795 Readable and editable output ( .yaml file with all recorded requests and responses in a one place) \u2795 Opportunity to pull stubs from the device and store them to the device \u2796 Doesn't maintain \u2796 Doesn't have request indexing \u2796 Doesn't keep http request time \u2796 Doesn't have manually mocking \u2796 Doesn't have smart indexing Solution from Revolut (Work in progress) Revolut prepares an open source library, which allows getting rid of all problems mentioned above and make adoption a way easier Problems Account preparation You need to prepare a user which matches the special condition you need to test. It can be still mocked manually or via specially created service, which does this job for you Core changes As soon as your root code base changed in terms of http requests, you probably would need to re-record some tests. That's why it's better to skip authorisation mode in the playback mode, to reduce such cases","title":"Mock network (automatically)"},{"location":"practices/network/#conclusion","text":"No perfect solution exists, it all depends on a number of factors, for example, the size of the team and how often the code base changes. It also depends on what goals you set in UI testing. Should I avoid real network testing in general? Depends on UI testing goals, project type and testing priorities. If it's important for you to have UI testing as a part of Pull Request or Release pipelines, then it's very important to put the stability of these tests first a place. Mocking the network, you sacrifice some uncaught bugs, but you get stability and speed in return. These tests will not catch a bug when something has changed on the backend, but you can test how your product is ready for a change contract and nullable/non-nullable fields. On the other hand, if you do not need a full automation and having UI testing as a part of it, and at the same time an option to use production backend, tests with a real network are all still have the right to life in your product. I have a small project. Which tool can I use for mock? If you have not a lot of http requests, like 5-20 on application start, MockWebServer will be pragmatic option I have a lot of http requests, my codebase changes really often Record/Replay practice will be useful in that case. Check OkReplay or Wiremock solution. Also, you can combine approaches: Record & Replay, and mock only specific requests. You also may check, what approaches are used by other companies","title":"Conclusion"},{"location":"practices/obfuscated_build/","text":"Testing obfuscated build How to test the closest possible to production build. Problem We work on debug builds and most often see debug builds and our UI tests run on debug builds. But user has release build. It is the same build apart from optimisations we do to reduce binary size and protect our apps identity. Any problems related to these optimisations are very rare but still would be good to catch them even before they hit beta. Solution Run UI tests on obfuscated build. For that we need to use keeper plugin. The reason is that Android Gradle Plugin doesn't include usages from androidTest sources and will throw out all code referenced by UI tests. So they won't work. We can use Keeper in two ways: Run UI tests on release build Can be done by adding following to build.gradle : android { testBuildType = \"release\" } or rather: android { if (hasProperty(\"testingMinimizedBuild\")) { testBuildType = \"release\" } } But this has downside: sometimes you separate code via build type folders. E.g. place dummy implementation in /debug/ and real implementation into /release/ source set to make sure debug code never gets into production builds. Or the same principle applied to dependencies like: debugImplementation 'com.facebook.flipper:flipper:0.154.0' debugImplementation 'com.facebook.soloader:soloader:0.10.1' releaseImplementation 'com.facebook.flipper:flipper-noop:0.154.0' If you have such configurations, it's not a way to go. Run obfuscation on debug build. Instead of running tests on release build type, we run them as usual on debug, but apply obfuscation to debug build via: buildTypes { debug { ... if (hasProperty(\"testingMinimizedBuild\")) { isMinifyEnabled = hasProperty(\"testingMinimizedBuild\") isShrinkResources = hasProperty(\"testingMinimizedBuild\") proguardFiles 'proguard-rules.pro' } } } Main trick Once we have build and minimize for tests we need to keep all needed classes. To do so we apply keeper plugin: if (hasProperty(\"testingMinimizedBuild\")) { apply plugin: \"com.slack.keeper\" } As you noted we do everything if some property(eg hasProperty(\"testingMinimizedBuild\") ). This way we can run UI tests normally and to run tests on obfuscated build. To pass param to the build: ./gradlew assembleDebugAndroidTest -PtestingMinimizedBuild Other tricks R8 repo Keeper adds R8 repo on project level so if your project uses repositoriesMode.set(RepositoriesMode.PREFER_SETTINGS) it will fail the build. What you need to do is to tell keeper not to add any repos and do it yourself: keeper { automaticR8RepoManagement = false } ... repositories { ... maven { setUrl(\"http://storage.googleapis.com/r8-releases/raw\") } } Memory and time Obviously build will take longer time depending on project size. But you also need to increase heap memory for JVM, otherwise you'll get lots OOMs. You can either do it in gradle.properties file: org.gradle.jvmargs=-Xmx16G -XX:+UseParallelGC -Dfile.encoding=UTF-8 Or to give more memory only for those runs(your final command may look like): ./gradlew assembleDebugAndroidTest -PtestingMinimizedBuild \"-Dorg.gradle.jvmargs=-Xmx16G -XX:+UseParallelGC\" -Dfile.encoding=UTF-8 Note : double quotes for \"-Dorg.gradle.jvmargs=-Xmx16G -XX:+UseParallelGC\" otherwise gradle may be unhappy with incorrect org.gradle.jvmargs . Additional Proguard rules Depending on your UI tests you may want to disable obfuscation of certain classes in addition to you main Proguard rules so that your test code can find needed stuff. proguard-debug-r8.pro # Make UI tests able to find needed stuff. -keep class org.yaml.** { *; } -keep class okreplay.** { *; } -keepattributes InnerClasses -keep class **.R -keep class **.R$* { <fields>; } And in: buildTypes { release { minifyEnabled true proguardFiles 'proguard-rules.pro' } release { if (hasProperty(\"testingMinimizedBuild\")) { minifyEnabled true proguardFiles 'proguard-rules.pro', 'proguard-debug-r8.pro' // here we extend proguard with our test specific rules file } } } AGP version If your Android Gradle Plugin version is less than 7.1.0 than you need not the latest version of keeper. You need 0.11.2 . This is because of new gradle API through which you apply the plugin. Also on different versions of AGP work different R8. If something doesn't work(you see some PrintUses stack trace) you may want to try new R8 TraceReferences API(worked for us on AGP 7.1.+): keeper { traceReferences() } Otherwise you may want to try different version of R8. Look for tags here . More here . Further reading: Keeper advanced configuration and reading source code. Testing minimized build at Avito","title":"Testing obfuscated build"},{"location":"practices/obfuscated_build/#testing-obfuscated-build","text":"How to test the closest possible to production build.","title":"Testing obfuscated build"},{"location":"practices/obfuscated_build/#problem","text":"We work on debug builds and most often see debug builds and our UI tests run on debug builds. But user has release build. It is the same build apart from optimisations we do to reduce binary size and protect our apps identity. Any problems related to these optimisations are very rare but still would be good to catch them even before they hit beta.","title":"Problem"},{"location":"practices/obfuscated_build/#solution","text":"Run UI tests on obfuscated build. For that we need to use keeper plugin. The reason is that Android Gradle Plugin doesn't include usages from androidTest sources and will throw out all code referenced by UI tests. So they won't work. We can use Keeper in two ways:","title":"Solution"},{"location":"practices/obfuscated_build/#run-ui-tests-on-release-build","text":"Can be done by adding following to build.gradle : android { testBuildType = \"release\" } or rather: android { if (hasProperty(\"testingMinimizedBuild\")) { testBuildType = \"release\" } } But this has downside: sometimes you separate code via build type folders. E.g. place dummy implementation in /debug/ and real implementation into /release/ source set to make sure debug code never gets into production builds. Or the same principle applied to dependencies like: debugImplementation 'com.facebook.flipper:flipper:0.154.0' debugImplementation 'com.facebook.soloader:soloader:0.10.1' releaseImplementation 'com.facebook.flipper:flipper-noop:0.154.0' If you have such configurations, it's not a way to go.","title":"Run UI tests on release build"},{"location":"practices/obfuscated_build/#run-obfuscation-on-debug-build","text":"Instead of running tests on release build type, we run them as usual on debug, but apply obfuscation to debug build via: buildTypes { debug { ... if (hasProperty(\"testingMinimizedBuild\")) { isMinifyEnabled = hasProperty(\"testingMinimizedBuild\") isShrinkResources = hasProperty(\"testingMinimizedBuild\") proguardFiles 'proguard-rules.pro' } } }","title":"Run obfuscation on debug build."},{"location":"practices/obfuscated_build/#main-trick","text":"Once we have build and minimize for tests we need to keep all needed classes. To do so we apply keeper plugin: if (hasProperty(\"testingMinimizedBuild\")) { apply plugin: \"com.slack.keeper\" } As you noted we do everything if some property(eg hasProperty(\"testingMinimizedBuild\") ). This way we can run UI tests normally and to run tests on obfuscated build. To pass param to the build: ./gradlew assembleDebugAndroidTest -PtestingMinimizedBuild","title":"Main trick"},{"location":"practices/obfuscated_build/#other-tricks","text":"","title":"Other tricks"},{"location":"practices/obfuscated_build/#r8-repo","text":"Keeper adds R8 repo on project level so if your project uses repositoriesMode.set(RepositoriesMode.PREFER_SETTINGS) it will fail the build. What you need to do is to tell keeper not to add any repos and do it yourself: keeper { automaticR8RepoManagement = false } ... repositories { ... maven { setUrl(\"http://storage.googleapis.com/r8-releases/raw\") } }","title":"R8 repo"},{"location":"practices/obfuscated_build/#memory-and-time","text":"Obviously build will take longer time depending on project size. But you also need to increase heap memory for JVM, otherwise you'll get lots OOMs. You can either do it in gradle.properties file: org.gradle.jvmargs=-Xmx16G -XX:+UseParallelGC -Dfile.encoding=UTF-8 Or to give more memory only for those runs(your final command may look like): ./gradlew assembleDebugAndroidTest -PtestingMinimizedBuild \"-Dorg.gradle.jvmargs=-Xmx16G -XX:+UseParallelGC\" -Dfile.encoding=UTF-8 Note : double quotes for \"-Dorg.gradle.jvmargs=-Xmx16G -XX:+UseParallelGC\" otherwise gradle may be unhappy with incorrect org.gradle.jvmargs .","title":"Memory and time"},{"location":"practices/obfuscated_build/#additional-proguard-rules","text":"Depending on your UI tests you may want to disable obfuscation of certain classes in addition to you main Proguard rules so that your test code can find needed stuff. proguard-debug-r8.pro # Make UI tests able to find needed stuff. -keep class org.yaml.** { *; } -keep class okreplay.** { *; } -keepattributes InnerClasses -keep class **.R -keep class **.R$* { <fields>; } And in: buildTypes { release { minifyEnabled true proguardFiles 'proguard-rules.pro' } release { if (hasProperty(\"testingMinimizedBuild\")) { minifyEnabled true proguardFiles 'proguard-rules.pro', 'proguard-debug-r8.pro' // here we extend proguard with our test specific rules file } } }","title":"Additional Proguard rules"},{"location":"practices/obfuscated_build/#agp-version","text":"If your Android Gradle Plugin version is less than 7.1.0 than you need not the latest version of keeper. You need 0.11.2 . This is because of new gradle API through which you apply the plugin. Also on different versions of AGP work different R8. If something doesn't work(you see some PrintUses stack trace) you may want to try new R8 TraceReferences API(worked for us on AGP 7.1.+): keeper { traceReferences() } Otherwise you may want to try different version of R8. Look for tags here . More here .","title":"AGP version"},{"location":"practices/obfuscated_build/#further-reading","text":"Keeper advanced configuration and reading source code. Testing minimized build at Avito","title":"Further reading:"},{"location":"practices/page_object/","text":"Page object How to make tests more clear and readable? Problem There is a lot of ViewMatchers and so on in our tests once we need to find exact View . Imagine that we do have hundreds of tests that starts with pressing same button. What will be if that button would change its id? We would change ViewMatcher inside every single test. Also there is a problem if our View should be accessed with a lot of ViewMatchers used (for example when that View is a child of RecyclerView ) What should we do in the above cases? May we should extract this View to another abstraction? Solution: Page Object Pattern Actually that pattern came to Android world from Web testing. This is how PageObject determined by one of its creator: The basic rule of thumb for a page object is that it should allow a software client to do anything and see anything that a human can. It should also provide an interface that's easy to program to and hides the underlying widgetry in the window. So to access a text field you should have accessor methods that take and return a string, check boxes should use booleans, and buttons should be represented by action oriented method names. www.martinfowler.com/bliki/PageObject.html Example We do have some screen with 3 Buttons Let's write some test for that screen with plain espresso @Test fun testFirstFeature () { onView ( withId ( R . id . toFirstFeature )) . check ( ViewAssertions . matches ( ViewMatchers . withEffectiveVisibility ( ViewMatchers . Visibility . VISIBLE ))) onView ( withId ( R . id . toFirstFeature )). perform ( click ()) } That test finds one of our button then checks its visibility and after that performs usual click. Main problem here \u2014 it's not easy to read. What do we want to achieve with PageObject? Ideally we want to have something like @Test fun testFirstFeature () { MainScreen . firstFeatureButton . isVisible () MainScreen . firstFeatureButton . click () } What is the difference we can see here? We use ViewMatcher inside of our test We added MainScreen abstraction that actually is a PageObject of screen provided in example isVisible() and click() are extensions (for example) As you can see that change made our code more clear and readable. And that happened even with one single test that checks visibility of button and clicks on it. Just imagine how much effort that pattern will bring to your codebase in case of hundreds tests written with PageObject Instead of writing your own implementation of PageObject pattern Just take a look for Kakao library it has a modern Kotlin DSL implementation of PageObject pattern A lot of useful classes for interact with. For example, same test for our screen written with Kakao library will look like @Test fun testFirstFeature () { mainScreen { toFirstFeatureButton { isVisible () click () } } } Conclusion PageObject pattern helps us to: \u2795 Remove duplicates of ViewMatchers from tests \u2795 Once we change id/text/whatever of View we should change it only in one place of PageObject class \u2795 New abstraction to make code more readable and clear","title":"Page object"},{"location":"practices/page_object/#page-object","text":"How to make tests more clear and readable?","title":"Page object"},{"location":"practices/page_object/#problem","text":"There is a lot of ViewMatchers and so on in our tests once we need to find exact View . Imagine that we do have hundreds of tests that starts with pressing same button. What will be if that button would change its id? We would change ViewMatcher inside every single test. Also there is a problem if our View should be accessed with a lot of ViewMatchers used (for example when that View is a child of RecyclerView ) What should we do in the above cases? May we should extract this View to another abstraction?","title":"Problem"},{"location":"practices/page_object/#solution-page-object-pattern","text":"Actually that pattern came to Android world from Web testing. This is how PageObject determined by one of its creator: The basic rule of thumb for a page object is that it should allow a software client to do anything and see anything that a human can. It should also provide an interface that's easy to program to and hides the underlying widgetry in the window. So to access a text field you should have accessor methods that take and return a string, check boxes should use booleans, and buttons should be represented by action oriented method names. www.martinfowler.com/bliki/PageObject.html","title":"Solution: Page Object Pattern"},{"location":"practices/page_object/#example","text":"We do have some screen with 3 Buttons","title":"Example"},{"location":"practices/page_object/#lets-write-some-test-for-that-screen-with-plain-espresso","text":"@Test fun testFirstFeature () { onView ( withId ( R . id . toFirstFeature )) . check ( ViewAssertions . matches ( ViewMatchers . withEffectiveVisibility ( ViewMatchers . Visibility . VISIBLE ))) onView ( withId ( R . id . toFirstFeature )). perform ( click ()) } That test finds one of our button then checks its visibility and after that performs usual click. Main problem here \u2014 it's not easy to read.","title":"Let's write some test for that screen with plain espresso"},{"location":"practices/page_object/#what-do-we-want-to-achieve-with-pageobject","text":"Ideally we want to have something like @Test fun testFirstFeature () { MainScreen . firstFeatureButton . isVisible () MainScreen . firstFeatureButton . click () } What is the difference we can see here? We use ViewMatcher inside of our test We added MainScreen abstraction that actually is a PageObject of screen provided in example isVisible() and click() are extensions (for example) As you can see that change made our code more clear and readable. And that happened even with one single test that checks visibility of button and clicks on it. Just imagine how much effort that pattern will bring to your codebase in case of hundreds tests written with PageObject","title":"What do we want to achieve with PageObject?"},{"location":"practices/page_object/#instead-of-writing-your-own-implementation-of-pageobject-pattern","text":"Just take a look for Kakao library it has a modern Kotlin DSL implementation of PageObject pattern A lot of useful classes for interact with. For example, same test for our screen written with Kakao library will look like @Test fun testFirstFeature () { mainScreen { toFirstFeatureButton { isVisible () click () } } }","title":"Instead of writing your own implementation of PageObject pattern"},{"location":"practices/page_object/#conclusion","text":"PageObject pattern helps us to: \u2795 Remove duplicates of ViewMatchers from tests \u2795 Once we change id/text/whatever of View we should change it only in one place of PageObject class \u2795 New abstraction to make code more readable and clear","title":"Conclusion"},{"location":"practices/shared_test_components/","text":"Sharing ui components among tests As the application grows, sooner or later the question of using a design system and common components arises. The design system forces us to think not with ready-made components provided by the Android SDK , but with our own, which are reused in different parts of the application and significantly speed up the development of new functionalities. Problem Design system introduction raises a lot of custom views usage. Let's review a typical PageObject implementation, if you have a lot of custom views (Including our own toolbar implementation from the design system) : object MainScreen : KaspressoScreen < MainScreen > { private val tvToolbarTitle = KTextView { withParent { withId ( R . id . toolbar_root ) } withId ( R . id . toolbar_title ) } private val ivToolbarImage = KTextView { withParent { withId ( R . id . toolbar_root ) } withId ( R . id . toolbar_title ) } private val recycler = KRecyclerView ( { withId ( R . id . recycler_view ) }, itemTypeBuilder = { itemType ( :: HeaderItem ) itemType ( :: ContactItem ) } fun clickContact ( name : String ) { recycler . childWith < ContactItem > { withText ( name ) } perform { isVisible () click () } } fun assertTitleVisible () { tvToolbarTitle . isVisible () } fun assertImageVisible () { ivToolbarImage . isVisible () } private class ContactItem ( parent : Matcher < View > ) : KRecyclerItem < MainItem > ( parent ) { val title : KTextView = KTextView ( parent ) { withId ( R . id . tv_header ) } } private class TitleItem ( parent : Matcher < View > ) : KRecyclerItem < CheckBoxItem > ( parent ) { val tvHeader : KTextView = KTextView { withId ( R . id . tv_header ) } } } We may find the next issues: Readability In that case really easy to have a mistake with proper matchers Hard to support Changing a component can break all use cases in tests. Imagine, ToolbarView uses in a hundred tests and described in each PageObject differently, all of them may be broken Time consuming To write such matchers, you need to spend the required amount of time. Developers don't really like writing tests. Ideally, this process should be simplified to a minimum. Solution The solution to this problem is exactly the same as for real components. Each component of the design system must have its own component for UI testing, which encapsulates all matchers inside. Let's see what our PageObject looks like using these components: object MainScreen : KaspressoScreen < MainScreen > { // located now in the test design system module private val toolbar = TToolbarView { withId ( R . id . toolbar_component ) } private val recycler = KRecyclerView ( { withId ( R . id . recycler_view ) }, itemTypeBuilder = { itemType ( :: TRowItem ) // located now in the test design system module itemType ( :: THeaderItem ) // located now in the test design system module } fun assertTitleVisible () { toolbar . title . isVisible () } fun assertImageVisible () { toolbar . image . isVisible () } fun clickContact ( name : String ) { recycler . childWith < TRowItem > { withText ( name ) } perform { isVisible () click () } } } Such components require a minimum of developer effort to write a PageObject and solve all the above issues Where to locate such components? If there is a system in the product, it is most likely located in one or more separated gradle modules. Tests components are recommended to be located in a separate module, which will be used only in the instrumented testing and will be connected using androidTestImplementation in gradle . For instance, if the product design system module is called design_system , then in tests you can use the prefix ui_tests_design_system However, you need also make sure that when adding/modifying a new/old component, the test component is also created/modified. This can be guaranteed with code analysis tools such as Danger or Detect . R files problem When you move test view components to a separate module, if you still use transitive R files in your project, you will have a problem with a wrong ids generation. Transitive R files are still used by default. Internally, each module generates own R file and re-generate each dependant resources from other modules. Unfortunately, using ids from production module in the test module, which connected by androidTestImplementation will raise an issue: dependant resources will be generated wrongly and view won't be found in the test. (Most likely this is a bug in the Android SDK) This problem can be easily solved by migration to Non-transitive R files , where we can use already generated R files by other modules. You can read the details about R files and that problem here: The past and the future of Android R class","title":"Sharing ui components among tests"},{"location":"practices/shared_test_components/#sharing-ui-components-among-tests","text":"As the application grows, sooner or later the question of using a design system and common components arises. The design system forces us to think not with ready-made components provided by the Android SDK , but with our own, which are reused in different parts of the application and significantly speed up the development of new functionalities.","title":"Sharing ui components among tests"},{"location":"practices/shared_test_components/#problem","text":"Design system introduction raises a lot of custom views usage. Let's review a typical PageObject implementation, if you have a lot of custom views (Including our own toolbar implementation from the design system) : object MainScreen : KaspressoScreen < MainScreen > { private val tvToolbarTitle = KTextView { withParent { withId ( R . id . toolbar_root ) } withId ( R . id . toolbar_title ) } private val ivToolbarImage = KTextView { withParent { withId ( R . id . toolbar_root ) } withId ( R . id . toolbar_title ) } private val recycler = KRecyclerView ( { withId ( R . id . recycler_view ) }, itemTypeBuilder = { itemType ( :: HeaderItem ) itemType ( :: ContactItem ) } fun clickContact ( name : String ) { recycler . childWith < ContactItem > { withText ( name ) } perform { isVisible () click () } } fun assertTitleVisible () { tvToolbarTitle . isVisible () } fun assertImageVisible () { ivToolbarImage . isVisible () } private class ContactItem ( parent : Matcher < View > ) : KRecyclerItem < MainItem > ( parent ) { val title : KTextView = KTextView ( parent ) { withId ( R . id . tv_header ) } } private class TitleItem ( parent : Matcher < View > ) : KRecyclerItem < CheckBoxItem > ( parent ) { val tvHeader : KTextView = KTextView { withId ( R . id . tv_header ) } } } We may find the next issues: Readability In that case really easy to have a mistake with proper matchers Hard to support Changing a component can break all use cases in tests. Imagine, ToolbarView uses in a hundred tests and described in each PageObject differently, all of them may be broken Time consuming To write such matchers, you need to spend the required amount of time. Developers don't really like writing tests. Ideally, this process should be simplified to a minimum.","title":"Problem"},{"location":"practices/shared_test_components/#solution","text":"The solution to this problem is exactly the same as for real components. Each component of the design system must have its own component for UI testing, which encapsulates all matchers inside. Let's see what our PageObject looks like using these components: object MainScreen : KaspressoScreen < MainScreen > { // located now in the test design system module private val toolbar = TToolbarView { withId ( R . id . toolbar_component ) } private val recycler = KRecyclerView ( { withId ( R . id . recycler_view ) }, itemTypeBuilder = { itemType ( :: TRowItem ) // located now in the test design system module itemType ( :: THeaderItem ) // located now in the test design system module } fun assertTitleVisible () { toolbar . title . isVisible () } fun assertImageVisible () { toolbar . image . isVisible () } fun clickContact ( name : String ) { recycler . childWith < TRowItem > { withText ( name ) } perform { isVisible () click () } } } Such components require a minimum of developer effort to write a PageObject and solve all the above issues","title":"Solution"},{"location":"practices/shared_test_components/#where-to-locate-such-components","text":"If there is a system in the product, it is most likely located in one or more separated gradle modules. Tests components are recommended to be located in a separate module, which will be used only in the instrumented testing and will be connected using androidTestImplementation in gradle . For instance, if the product design system module is called design_system , then in tests you can use the prefix ui_tests_design_system However, you need also make sure that when adding/modifying a new/old component, the test component is also created/modified. This can be guaranteed with code analysis tools such as Danger or Detect .","title":"Where to locate such components?"},{"location":"practices/shared_test_components/#r-files-problem","text":"When you move test view components to a separate module, if you still use transitive R files in your project, you will have a problem with a wrong ids generation. Transitive R files are still used by default. Internally, each module generates own R file and re-generate each dependant resources from other modules. Unfortunately, using ids from production module in the test module, which connected by androidTestImplementation will raise an issue: dependant resources will be generated wrongly and view won't be found in the test. (Most likely this is a bug in the Android SDK) This problem can be easily solved by migration to Non-transitive R files , where we can use already generated R files by other modules. You can read the details about R files and that problem here: The past and the future of Android R class","title":"R files problem"},{"location":"practices/state_clearing/","text":"State clearing This question appears as soon as you need to run more than 1 ui test. Problem We run Test1 , it performs some http requests, saves some data to files and databases. When Test1 is finished, Test2 will be launched. However, Test1 left some data on the device which can be a reason of Test2 failing. Solution \u2014 clear the data before each test 1. Clearing within a process In this case, we don't kill our application process, and we have 2 options here: Use component from a real code base @Before fun setUp () { DI . provideLogoutCleanerInteractor (). clear () } The same component which clears data (For instance, while logout) . It should honestly clear everything in your application: Databases, Files, Preferences and Runtime cache, and should be executed before each test. Danger This solution is a bottleneck and it's better to avoid it at all. If LogoutCleaner is broken, all of the tests will be failed. Clear internal storage All cache in an android application is stored in the internal storage: /data/data/packagename/ This storage is our application sandbox and can be accessed without any permission. Basic idea is to avoid using components from a real code base. Instead of them, use some tests rules which do the job for us. @get : Rule val clearPreferenceRule = ClearDatabaseRule () @get : Rule val clearFilesRule = ClearFilesRule () @get : Rule val clearFilesRule = ClearPreferencesRule () They have already been implemented in Barista library, you can find them here Warning This solution won't in 100% of cases: You may have runtime cache, which can also affect your tests Test or application process may crash and prevent the launch of next tests Conclusion These are pros/cons for both solutions which don't kill the process: \u2795 Fast implementation \u2795 Fast execution in the same process \u2796 Don't give you any guarantee that your app will be cleared properly \u2796 Application or Test process killing will break tests execution \u2796 Can be a bottleneck Use these solutions only as a temp workaround, because it won't work on perspective in huge projects 2. Clearing package data Our aim is to simulate the same behavior as when user presses the clear data button in application settings. Application process will be cleared in that case, our application will be started in a cold start. Orchestrator Basically, you can achieve an isolated state, if you execute your tests like this: adb shell am instrument -c TestClass#method1 -w com.package.name/junitRunnerClass adb pm clear adb shell am instrument -c TestClass#method2 -w com.package.name/junitRunnerClass adb pm clear Each test should be executed in an isolated instrumented process and junit reports should be merged into a big one report when all tests are finished. That's the common idea of Orchestrator . It's just an apk which consist of only several classes and runs tests and clears data, as described above. You should install an orchestrator along with application.apk and instrumented.apk on the device. However, it's not the end. Orchestrator should somehow execute adb commands. Under the hood, it uses special services. It's just a shell client and should be installed to the device. An official documentation and guide how to start with Orchestrator Warning Despite the fact that it does the job, this solution looks overcomplicated: We need to install +2 different apk to each emulator We delegate this job to the device instead of host machine. Devices are less reliable than host pc Other solutions It's also possible to clear package data by using 3rd party test runners , like Marathon, Avito-Runner or Flank. Marathon and Avito-Runner clear package data without an orchestrator. They delegate this logic to a host machine Conclusion These are pros/cons for an orchestrator and 3rd party test runners solution: \u2795 Does the job for us in 100% \u2796 Slow execution (can take 10+ seconds and depends on apk size) \u2796 Orchestrator \u2014 over-complicated Each adb pm clear takes some time and depends on apk size. Below you may see some gaps between the tests which represent such a delay Success Only package clear can guarantee that your data will be celared properly. Marathon and Avito-Runner provide the easiest way to clear application data. You can set them just by one flag in configuration They don't use orchestrator under the hood","title":"State clearing"},{"location":"practices/state_clearing/#state-clearing","text":"This question appears as soon as you need to run more than 1 ui test.","title":"State clearing"},{"location":"practices/state_clearing/#problem","text":"We run Test1 , it performs some http requests, saves some data to files and databases. When Test1 is finished, Test2 will be launched. However, Test1 left some data on the device which can be a reason of Test2 failing. Solution \u2014 clear the data before each test","title":"Problem"},{"location":"practices/state_clearing/#1-clearing-within-a-process","text":"In this case, we don't kill our application process, and we have 2 options here:","title":"1. Clearing within a process"},{"location":"practices/state_clearing/#use-component-from-a-real-code-base","text":"@Before fun setUp () { DI . provideLogoutCleanerInteractor (). clear () } The same component which clears data (For instance, while logout) . It should honestly clear everything in your application: Databases, Files, Preferences and Runtime cache, and should be executed before each test. Danger This solution is a bottleneck and it's better to avoid it at all. If LogoutCleaner is broken, all of the tests will be failed.","title":"Use component from a real code base "},{"location":"practices/state_clearing/#clear-internal-storage","text":"All cache in an android application is stored in the internal storage: /data/data/packagename/ This storage is our application sandbox and can be accessed without any permission. Basic idea is to avoid using components from a real code base. Instead of them, use some tests rules which do the job for us. @get : Rule val clearPreferenceRule = ClearDatabaseRule () @get : Rule val clearFilesRule = ClearFilesRule () @get : Rule val clearFilesRule = ClearPreferencesRule () They have already been implemented in Barista library, you can find them here Warning This solution won't in 100% of cases: You may have runtime cache, which can also affect your tests Test or application process may crash and prevent the launch of next tests","title":"Clear internal storage  "},{"location":"practices/state_clearing/#conclusion","text":"These are pros/cons for both solutions which don't kill the process: \u2795 Fast implementation \u2795 Fast execution in the same process \u2796 Don't give you any guarantee that your app will be cleared properly \u2796 Application or Test process killing will break tests execution \u2796 Can be a bottleneck Use these solutions only as a temp workaround, because it won't work on perspective in huge projects","title":"Conclusion"},{"location":"practices/state_clearing/#2-clearing-package-data","text":"Our aim is to simulate the same behavior as when user presses the clear data button in application settings. Application process will be cleared in that case, our application will be started in a cold start.","title":"2. Clearing package data"},{"location":"practices/state_clearing/#orchestrator","text":"Basically, you can achieve an isolated state, if you execute your tests like this: adb shell am instrument -c TestClass#method1 -w com.package.name/junitRunnerClass adb pm clear adb shell am instrument -c TestClass#method2 -w com.package.name/junitRunnerClass adb pm clear Each test should be executed in an isolated instrumented process and junit reports should be merged into a big one report when all tests are finished. That's the common idea of Orchestrator . It's just an apk which consist of only several classes and runs tests and clears data, as described above. You should install an orchestrator along with application.apk and instrumented.apk on the device. However, it's not the end. Orchestrator should somehow execute adb commands. Under the hood, it uses special services. It's just a shell client and should be installed to the device. An official documentation and guide how to start with Orchestrator Warning Despite the fact that it does the job, this solution looks overcomplicated: We need to install +2 different apk to each emulator We delegate this job to the device instead of host machine. Devices are less reliable than host pc","title":"Orchestrator"},{"location":"practices/state_clearing/#other-solutions","text":"It's also possible to clear package data by using 3rd party test runners , like Marathon, Avito-Runner or Flank. Marathon and Avito-Runner clear package data without an orchestrator. They delegate this logic to a host machine","title":"Other solutions"},{"location":"practices/state_clearing/#conclusion_1","text":"These are pros/cons for an orchestrator and 3rd party test runners solution: \u2795 Does the job for us in 100% \u2796 Slow execution (can take 10+ seconds and depends on apk size) \u2796 Orchestrator \u2014 over-complicated Each adb pm clear takes some time and depends on apk size. Below you may see some gaps between the tests which represent such a delay Success Only package clear can guarantee that your data will be celared properly. Marathon and Avito-Runner provide the easiest way to clear application data. You can set them just by one flag in configuration They don't use orchestrator under the hood","title":"Conclusion"},{"location":"practices/test_runners_review/","text":"Test runners Test runner is responsible for tests run and providing test result for us. AndroidJunitRunner \u2014 Official solution and low-level instrument. It requires a lot of effort from engineers to run tests on CI and make them stable. It's worth to mention \u2014 tools are getting better year by year. However, some basic functionality still doesn't work from the box properly. 1. Problems with AndroidJunitRunner: Overcomplicated solution with clearing It would be good to have only one flag which does application clearing for us. It exists, however to have scalability on CI and opportunity to use filters, you still have to install test-services.apk and orchestrator.apk to each device manually Impossibility to scale As soon as you started your tests, it's impossible to connect more devices to tests run on fly Impossibility to prevent flakiness Flakiness is one of the main problems in instrumented testing. Test runner should play role as a latest flakiness protection level, like support retries from the box or other strategies Impossibility to validate flakiness Flakiness can be validated by running each test multiple times, and if test pass N/N, it's not flaky. It would be great to launch each test 100 times by one command Poor test report Default test report doesn't show any useful information. As an engineer, I want to see a video of the test, logs and to make sure that test hasn't been retried. Otherwise, I'd like to see retries and each retry video and logs. Impossibility to retry It's possible to do only via special test rule which does retry for us. However, it's up to test runner to retry each test, as instrumented process can be crashed and device less reliable than host machine. Also, it should be possible to define maximum retry count: Imagine, your application crashed on start. We shouldn't retry each test in that case because there is no sense to overload build agents on CI. Impossibility to record a video It's possible to achieve and implement manually, however It would be really great to have such functionality from the box Almost all of that problems possible to solve, but it can take weeks or even months of your time. Beside running tests, you also need to care about writing tests which is challenging as well. It would be great to have that problems solved from the box 2. Open source test runners All of them used AndroidJunitRunner under the hood, as it's the only possibility tun run instrumented tests. 2.1 Marathon Powerful and the most pragmatic test runner. All you need to do it's just to connect devices to adb , and Marathon will do the whole job for you. \u2795 Stand-alone or Gradle Plugin \u2795 Easy data clearing (without an Orchestrator) \u2795 Flexible configuration with filters \u2795 Flakiness strategies \u2795 Dynamic test batching (test count/test duration) \u2795 Smart retries with a quotas \u2795 Screenshots & video out of the box \u2795 Improved test report with video & logs \u2795 Automatically rebalanced test execution if connecting/disconnecting devices on fly \u2795 Pull files from the device after test run, e.g. allure-kotlin \u2795 Basic Allure support out of the box \u2795 adb client ddmlib replacement: Adam \u2795 Cross-platform (iOS support) \u2795 Fragmented test execution (similar to AOSP's sharding): split large testing suites into multiple CI builds \u2795 Parallel execution of parameterised tests \u2795 Interactions with adb/emulator from within a test (e.g. fake fingerprint or GPS) \u2795 Code coverage support \u2795 Testing multi-module projects in one test run \u2795 Flakiness fixing mode to verify test passing probability improvements \u2796 Doesn't auto-scale devices (Marathon will utilise more devices in runtime if some other system connects more to the adb, but marathon itself will not spawn more emulators for you) \u2796 HTML report doesn't contain test retry information (but the Allure report does) \u2796 For complex test executions that solve test flakiness requires an installation of TSDB (InfluxDB or Graphite) Documentation 2.2 Avito Test Runner Powerful test runner. Works directly with Kubernetes \u2795 Easy data clearing (without an Orchestrator) \u2795 Auto-scaling on fly (There is a coroutine in the background which tries to connect more devices) \u2795 Retries \u2795 Good test report \u2795 Unit tests support \u2796 Complicated adoption \u2796 No stand-alone solution This test runner has been using by Avito company for 4+ years and runs thousands tests every day. It's not as powerful as Marathon, however it doesn't have an analogue in terms of auto scaling from the box. If you want to run your UI tests on pull requests in a large teams, this test runner is one of the best option. Engineers from Avito are ready to help with adoption. You can contact to Dmitriy Voronin Documentation 2.3 Fork \u2795 Retries \u2795 Filters \u2795 Good test report with flakiness \u2795 Stand-alone or Gradle plugin \u2795 Performance profiling with Chimprunner \u2796 Data clearing \u2796 Not actively maintains 2.4 Flank \u2795 Don't need to care about Device infrastructure \u2795 Easy device data clearing (With an Orchestrator internally) \u2795 Stand-alone or Gradle plugin \u2795 Huge variety for choosing devices (Emulators/Real devices) \u2795 Good test report \u2795 Additional gradle plugin: Fladle \u2796 Paid service 2.6 Spoon Deprecated and not maintained anymore. Do not use it 2.7 Composer Deprecated and not maintained anymore. Do not use it Conclusion There is no right and wrong choice. As you see, all test runners have something unique. Keep in mind that nowadays Marathon is the most powerful test runner and will be pragmatic choice for any team. From the other side, if you need to have a result right here and right now and you are ready to pay, Flank or Fladle will be pragmatic options.","title":"Test runners"},{"location":"practices/test_runners_review/#test-runners","text":"Test runner is responsible for tests run and providing test result for us. AndroidJunitRunner \u2014 Official solution and low-level instrument. It requires a lot of effort from engineers to run tests on CI and make them stable. It's worth to mention \u2014 tools are getting better year by year. However, some basic functionality still doesn't work from the box properly.","title":"Test runners"},{"location":"practices/test_runners_review/#1-problems-with-androidjunitrunner","text":"Overcomplicated solution with clearing It would be good to have only one flag which does application clearing for us. It exists, however to have scalability on CI and opportunity to use filters, you still have to install test-services.apk and orchestrator.apk to each device manually Impossibility to scale As soon as you started your tests, it's impossible to connect more devices to tests run on fly Impossibility to prevent flakiness Flakiness is one of the main problems in instrumented testing. Test runner should play role as a latest flakiness protection level, like support retries from the box or other strategies Impossibility to validate flakiness Flakiness can be validated by running each test multiple times, and if test pass N/N, it's not flaky. It would be great to launch each test 100 times by one command Poor test report Default test report doesn't show any useful information. As an engineer, I want to see a video of the test, logs and to make sure that test hasn't been retried. Otherwise, I'd like to see retries and each retry video and logs. Impossibility to retry It's possible to do only via special test rule which does retry for us. However, it's up to test runner to retry each test, as instrumented process can be crashed and device less reliable than host machine. Also, it should be possible to define maximum retry count: Imagine, your application crashed on start. We shouldn't retry each test in that case because there is no sense to overload build agents on CI. Impossibility to record a video It's possible to achieve and implement manually, however It would be really great to have such functionality from the box Almost all of that problems possible to solve, but it can take weeks or even months of your time. Beside running tests, you also need to care about writing tests which is challenging as well. It would be great to have that problems solved from the box","title":"1. Problems with AndroidJunitRunner:"},{"location":"practices/test_runners_review/#2-open-source-test-runners","text":"All of them used AndroidJunitRunner under the hood, as it's the only possibility tun run instrumented tests.","title":"2. Open source test runners"},{"location":"practices/test_runners_review/#21-marathon","text":"Powerful and the most pragmatic test runner. All you need to do it's just to connect devices to adb , and Marathon will do the whole job for you. \u2795 Stand-alone or Gradle Plugin \u2795 Easy data clearing (without an Orchestrator) \u2795 Flexible configuration with filters \u2795 Flakiness strategies \u2795 Dynamic test batching (test count/test duration) \u2795 Smart retries with a quotas \u2795 Screenshots & video out of the box \u2795 Improved test report with video & logs \u2795 Automatically rebalanced test execution if connecting/disconnecting devices on fly \u2795 Pull files from the device after test run, e.g. allure-kotlin \u2795 Basic Allure support out of the box \u2795 adb client ddmlib replacement: Adam \u2795 Cross-platform (iOS support) \u2795 Fragmented test execution (similar to AOSP's sharding): split large testing suites into multiple CI builds \u2795 Parallel execution of parameterised tests \u2795 Interactions with adb/emulator from within a test (e.g. fake fingerprint or GPS) \u2795 Code coverage support \u2795 Testing multi-module projects in one test run \u2795 Flakiness fixing mode to verify test passing probability improvements \u2796 Doesn't auto-scale devices (Marathon will utilise more devices in runtime if some other system connects more to the adb, but marathon itself will not spawn more emulators for you) \u2796 HTML report doesn't contain test retry information (but the Allure report does) \u2796 For complex test executions that solve test flakiness requires an installation of TSDB (InfluxDB or Graphite) Documentation","title":"2.1 Marathon"},{"location":"practices/test_runners_review/#22-avito-test-runner","text":"Powerful test runner. Works directly with Kubernetes \u2795 Easy data clearing (without an Orchestrator) \u2795 Auto-scaling on fly (There is a coroutine in the background which tries to connect more devices) \u2795 Retries \u2795 Good test report \u2795 Unit tests support \u2796 Complicated adoption \u2796 No stand-alone solution This test runner has been using by Avito company for 4+ years and runs thousands tests every day. It's not as powerful as Marathon, however it doesn't have an analogue in terms of auto scaling from the box. If you want to run your UI tests on pull requests in a large teams, this test runner is one of the best option. Engineers from Avito are ready to help with adoption. You can contact to Dmitriy Voronin Documentation","title":"2.2 Avito Test Runner"},{"location":"practices/test_runners_review/#23-fork","text":"\u2795 Retries \u2795 Filters \u2795 Good test report with flakiness \u2795 Stand-alone or Gradle plugin \u2795 Performance profiling with Chimprunner \u2796 Data clearing \u2796 Not actively maintains","title":"2.3 Fork"},{"location":"practices/test_runners_review/#24-flank","text":"\u2795 Don't need to care about Device infrastructure \u2795 Easy device data clearing (With an Orchestrator internally) \u2795 Stand-alone or Gradle plugin \u2795 Huge variety for choosing devices (Emulators/Real devices) \u2795 Good test report \u2795 Additional gradle plugin: Fladle \u2796 Paid service","title":"2.4 Flank"},{"location":"practices/test_runners_review/#26-spoon","text":"Deprecated and not maintained anymore. Do not use it","title":"2.6 Spoon"},{"location":"practices/test_runners_review/#27-composer","text":"Deprecated and not maintained anymore. Do not use it","title":"2.7 Composer"},{"location":"practices/test_runners_review/#conclusion","text":"There is no right and wrong choice. As you see, all test runners have something unique. Keep in mind that nowadays Marathon is the most powerful test runner and will be pragmatic choice for any team. From the other side, if you need to have a result right here and right now and you are ready to pay, Flank or Fladle will be pragmatic options.","title":"Conclusion"}]}